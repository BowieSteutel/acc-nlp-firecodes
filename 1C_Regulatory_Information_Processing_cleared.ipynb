{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CZn_dy6swDtP",
        "V0HTiGcmsSa6",
        "jU6CXz-fZQfk",
        "H5YF2LcIGw-f",
        "Yh-Cu5argY2g",
        "dOfm28o1gVhm"
      ],
      "toc_visible": true,
      "mount_file_id": "1ACgGT6tgLW_LY-Vfnrh4VOb395hlLZgM",
      "authorship_tag": "ABX9TyNKA+UuZ01QCBZKuxc9Lezo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BowieSteutel/acc-nlp-firecodes/blob/main/1C_Regulatory_Information_Processing_cleared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Module 1C - Regulatory Information Extraction & Semantic Parsing**\n",
        "\n"
      ],
      "metadata": {
        "id": "dLp-pF81dzgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This module works with CSV files and ontologies.\n",
        "\n",
        "The CSV file of the regulations to be processed should include information about classifications, either manually assigned or included via the Regulation Classification module (1B)"
      ],
      "metadata": {
        "id": "_AfTlJglhhC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Load libraries**"
      ],
      "metadata": {
        "id": "CZn_dy6swDtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import re # for pattern matching regular expressions\n",
        "import pandas as pd # for dataframes\n",
        "import json # for export"
      ],
      "metadata": {
        "id": "kHpvRM3cveSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare rdflib (for SPARQL querying)\n",
        "!pip install rdflib --quiet\n",
        "import rdflib\n",
        "from rdflib import Graph, Namespace, URIRef, Literal\n",
        "from rdflib.namespace import RDF, RDFS, OWL"
      ],
      "metadata": {
        "id": "vdsVXGmk_fFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare spacy (for NLP pipeline)\n",
        "!pip install spacy --quiet\n",
        "import spacy\n",
        "from spacy import displacy # for visualization\n",
        "from spacy.matcher import Matcher # for pattern matching\n",
        "from spacy import Language # for custom pipeline components\n",
        "from spacy.tokens import Token, Span, Doc # for information extraction]\n",
        "\n",
        "# Prepare gliner_spacy (for NER)\n",
        "!pip install gliner-spacy --quiet\n",
        "from gliner_spacy.pipeline import GlinerSpacy"
      ],
      "metadata": {
        "id": "UvjNnKzdzqyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Load inputs**"
      ],
      "metadata": {
        "id": "V0HTiGcmsSa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Change root directory (update after downloading)\n",
        "\n",
        "root_directory = \"/content/drive/MyDrive/FINAL_CODE_THESIS\" #  @param {\"type\":\"string\", \"placeholder\":\"\"}\n",
        "import sys\n",
        "from pathlib import Path\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    %cd {root_directory}"
      ],
      "metadata": {
        "id": "rnsV4psd0yzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load files\n",
        "path_hierarchy = \"output/BBL_hier_elements.csv\" #  @param {type:\"string\", \"placeholder\":\"path to hierarchy, relative to file_path (csv)\"}\n",
        "path_subset_big = \"output/BBL_subset_final_big.csv\" #  @param {type:\"string\", \"placeholder\":\"path to subset, relative to file_path (csv)\"}\n",
        "path_subset_small = \"output/BBL_subset_final_small.csv\" #  @param {type:\"string\", \"placeholder\":\"path to subset, relative to file_path (csv)\"}\n",
        "\n",
        "df_hierarchy = pd.read_csv(path_hierarchy)#, encoding='windows-1252')\n",
        "df_subset_big = pd.read_csv(path_subset_big)#, encoding='windows-1252')\n",
        "df_subset_small = pd.read_csv(path_subset_small)#, encoding='windows-1252')\n",
        "df_subset_big.head()"
      ],
      "metadata": {
        "id": "JqbgsrLiq0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hier_codes = [reg['code'] for index, reg in df_hierarchy.iterrows()]\n",
        "hier_codes[:10]"
      ],
      "metadata": {
        "id": "IYwyqJPGn_KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Load default spaCy pipeline**\n",
        "\n",
        "*disabled NER (since custom NER will be added)*"
      ],
      "metadata": {
        "id": "QoeSUVimouAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load standard model\n",
        "\n",
        "# small:\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "\n",
        "# # medium:\n",
        "# !spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\", disable=[\"ner\"])\n",
        "\n",
        "# # large:\n",
        "# !spacy download en_core_web_lg\n",
        "# nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\"])\n",
        "\n",
        "# # Load transformer model for better parsing:\n",
        "# !pip install spacy-transformers # restart kernel afterwards?\n",
        "# import spacy_transformers\n",
        "\n",
        "# # !spacy download en_core_web_trf\n",
        "# nlp = spacy.load(\"en_core_web_trf\", disable=[\"ner\"])"
      ],
      "metadata": {
        "id": "RheUdrc4oneW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Load statistical NER (GLiNER)**\n",
        "\n",
        "***Requires Hugging Face token in environmental variables!***\n"
      ],
      "metadata": {
        "id": "hQA330d7nxUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare NER labels\n",
        "\n",
        "Defined based on literature and trial & error"
      ],
      "metadata": {
        "id": "Y2K_NnVVpRvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define NER labels\n",
        "labels = [\"building object\",  \"building component\",  #BEO\n",
        "          \"attribute\",\n",
        "          \"unit\", #\"measurement\", \"value\",\n",
        "          \"site\", \"building\", \"building storey\", \"building room\", \"building space\", #BOT\n",
        "          \"escape route\", \"fire compartment\", #FSE\n",
        "          \"fire system\", \"fire safety device\", #FSE\n",
        "          \"material\", \"layer\", \"geometry\",\n",
        "          \"classification\", # FSE\n",
        "          \"use function\", #\"area\", \"usage function\"\n",
        "          \"building type\", \"construction permanence\", #BBL\n",
        "          \"reference\", \"standard\", #REGULATIONS\n",
        "]\n",
        "\n",
        "\n",
        "# group NER labels\n",
        "NER_bot = [\"site\", \"building\", \"building storey\", \"building space\"]\n",
        "NER_custom_spaces = [\"fire compartment\", \"escape route\"]\n",
        "NER_spatial = NER_bot + NER_custom_spaces\n",
        "\n",
        "NER_rooms = [\"building_room\", \"use function\"] #\"building space\"?\n",
        "\n",
        "NER_elements = [\"building component\", \"building object\"]\n",
        "NER_MEP = [\"fire system\", \"fire safety device\"]\n",
        "NER_physical = NER_elements + NER_MEP\n",
        "#NER_physical = NER_elements + [\"fire safety device\"]\n",
        "#NER_physical = [\"building component\", \"building object\", \"fire safety device\"]\n",
        "\n",
        "NER_parts = [\"material\", \"layer\"]\n",
        "\n",
        "NER_props_default = [\"attribute\"]\n",
        "NER_props_custom = [ \"classification\"]# \"construction type\"? \"use function\"?\n",
        "NER_props = NER_props_default + NER_props_custom\n",
        "\n",
        "NER_quantity = [\"unit\"]\n",
        "NER_quality = [\"classification\", \"material\", \"use function\"]\n",
        "\n",
        "NER_references = [\"reference\", \"cross-reference\", \"standard\"] # includes cross-reference identified later"
      ],
      "metadata": {
        "id": "J23JwdDFpU7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add GLiNER to pipeline"
      ],
      "metadata": {
        "id": "TyFChwCVou8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_spacy_config = { \"gliner_model\": \"urchade/gliner_multi\",\n",
        "                            \"chunk_size\": 1000,\n",
        "                            \"labels\":labels,\n",
        "                            \"style\": \"ent\"}\n",
        "\n",
        "try:\n",
        "  ner = nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
        "except:\n",
        "  nlp.remove_pipe(\"gliner_spacy\")\n",
        "  ner = nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)"
      ],
      "metadata": {
        "id": "RXBbfjdXofFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare NER visualization"
      ],
      "metadata": {
        "id": "lysT_aDKpij0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labels & colors\n",
        "options = {\"colors\": {},\"ents\": []}\n",
        "\n",
        "ner_options = {\"colors\": {},\"ents\": []}\n",
        "\n",
        "# Extra labels for cross-references and quality (idenfitied with rule-based NER)\n",
        "ent_labels = custom_spacy_config.get(\"labels\")+['CROSS_REFERENCE', 'QUALITY']\n",
        "\n",
        "# Function for automatic label color generation below\n",
        "import colorsys\n",
        "def HSV2HEX(h, s, v):\n",
        "  (r, g, b) = colorsys.hsv_to_rgb(h, s, v)\n",
        "  return '#{:02x}{:02x}{:02x}'.format(int(255*r), int(255*g), int(255*b))\n",
        "def getDistinctColors(n):\n",
        "  huePartition = 1 / (n + 1)\n",
        "  # huePartition = 0.9 / (n + 1)\n",
        "  return (HSV2HEX(huePartition * value, .5, 1) for value in range(0, n))\n",
        "l_colors = [color for color in getDistinctColors(len(ent_labels))]\n",
        "\n",
        "for i, label in enumerate(ent_labels):\n",
        "  options[\"colors\"][label.lower()] = l_colors[i]\n",
        "  options[\"ents\"].append(label.lower())\n",
        "\n",
        "# Prepare NER visualization\n",
        "def display_ner_score(ner_doc):#, ner_options):\n",
        "  ents = ner_doc.ents\n",
        "  for ent in ents:\n",
        "    new_label = f\"{ent.label_} ({ent._.score:.0%})\"\n",
        "    ner_options[\"colors\"][new_label] = options[\"colors\"].get(ent.label_.lower())\n",
        "    ner_options[\"ents\"].append(new_label)\n",
        "    # ner_options['colors']['CROSS_REFERENCE (100%)'] = '#BBBBBB'\n",
        "    ent.label_ = new_label\n",
        "  ner_doc.ents = ents\n",
        "  displacy.render(ner_doc, style=\"ent\", options=ner_options)"
      ],
      "metadata": {
        "id": "AUOsooF_pFbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test NER on subset"
      ],
      "metadata": {
        "id": "EfzAcZbpp3XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df_subset_small['text_translated']:\n",
        "  doc = nlp(text)\n",
        "  #print(len(doc))\n",
        "  display_ner_score(doc)\n",
        "  #display_ner_score(doc, options)\n",
        "  #displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "rb3fdPhVXVn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df_subset_big['text_translated'][:10]:\n",
        "  doc = nlp(text)\n",
        "  #print(len(doc))\n",
        "  display_ner_score(doc)\n",
        "  #display_ner_score(doc, options)\n",
        "  #displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "e3YWVigZp4iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Load rule-based NER**\n"
      ],
      "metadata": {
        "id": "Il1w-NM_21yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-reference recognition"
      ],
      "metadata": {
        "id": "1VIUPbMTccOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare regex patterns"
      ],
      "metadata": {
        "id": "OKj_xs2v8vqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hier_labels = {\"chapter\": {\"re_untranslated\"    : r\"hoofdstuk(?:ken)?\",\n",
        "                          \"re_translated\"       : r\"chapter[s]?\",\n",
        "                          \"code\": \"C\"},\n",
        "               \"section\": {\"re_untranslated\"    : r\"afdeling(?:en)?\",\n",
        "                          \"re_translated\"       : r\"section[s]?\",\n",
        "                          \"code\": \"S\"},\n",
        "               \"paragraph\": {\"re_untranslated\"  : r\"(?:paragra(?:af|fen)|§)\",\n",
        "                          \"re_translated\"       : r\"paragraph[s]?\",\n",
        "                          \"code\": \"P\"},\n",
        "               \"article\": {\"re_untranslated\"    : r\"artikel(?:en|s)?\",\n",
        "                          \"re_translated\"       : r\"article[s]?\",\n",
        "                          \"code\": \"A\"},\n",
        "               \"sub-article\": {\"re_untranslated\": r\"\",\n",
        "                          \"re_translated\"       : r\"sub[-]?article[s]?\",\n",
        "                          \"code\": \"SUB\"},\n",
        "               \"table\": {\"re_untranslated\"      : r\"tabel(?:len)?\",\n",
        "                          \"re_translated\"       : r\"table[s]?\",\n",
        "                          \"code\": \"TABLE\"},\n",
        "               \"appendix\": {\"re_untranslated\"   : r\"bijlage(?:n|s)?\",\n",
        "                          \"re_translated\"       : r\"appendi(?:x|ces)\",\n",
        "                          \"code\": \"APPX\"},\n",
        "               \"figure\": {\"re_untranslated\"     : r\"figu(?:ur|en)?\",\n",
        "                          \"re_translated\"       : r\"figure[s]?\",\n",
        "                          \"code\": \"FIG\"}\n",
        "          }\n",
        "print([x for x in hier_labels])\n",
        "for x in hier_labels:\n",
        "    print(hier_labels[x])"
      ],
      "metadata": {
        "id": "5OxZeNrZxypX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tens_map_full = {\n",
        "    \"twenty\": \"20\", \"thirty\": \"30\", \"forty\": \"40\", \"fifty\": \"50\",\n",
        "    \"sixty\": \"60\", \"seventy\": \"70\", \"eighty\": \"80\", \"ninety\": \"90\",\n",
        "}\n",
        "\n",
        "tens_map_part = {\n",
        "    \"twenty-\": \"2\", \"thirty-\": \"3\", \"forty\": \"4\", \"fifty-\": \"5\",\n",
        "    \"sixty-\": \"6\", \"seventy-\": \"7\", \"eighty\": \"8\", \"ninety-\": \"9\",\n",
        "    \"twenty\": \"2\", \"thirty\": \"3\", \"forty\": \"4\", \"fifty\": \"5\",\n",
        "    \"sixty\": \"6\", \"seventy\": \"7\", \"eighty\": \"8\", \"ninety\": \"9\",\n",
        "}\n",
        "\n",
        "ordinal_map = {\n",
        "    \"first\": \"1st\", \"second\": \"2nd\", \"third\": \"3rd\", \"fourth\": \"4th\", \"fifth\": \"5th\",\n",
        "    \"sixth\": \"6th\", \"seventh\": \"7th\", \"eighth\": \"8th\", \"ninth\": \"9th\", \"tenth\": \"10th\",\n",
        "    \"eleventh\": \"11th\", \"twelfth\": \"12th\", \"thirteenth\": \"13th\", \"fourteenth\": \"14th\", \"fifteenth\": \"15th\",\n",
        "    \"sixteenth\": \"16th\", \"seventeenth\": \"17th\", \"eighteenth\": \"18th\", \"nineteenth\": \"19th\",\n",
        "    \"twentieth\": \"20th\", \"thirtieth\": \"30th\", \"fortieth\": \"40th\", \"fiftieth\": \"50th\",\n",
        "    \"sixtieth\": \"60th\", \"seventieth\": \"70th\", \"eightieth\": \"80th\", \"ninetieth\": \"90th\",\n",
        "}\n",
        "\n",
        "numerical_map = {\n",
        "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\",\n",
        "    \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\",\n",
        "    \"eleven\" : \"11\", \"twelve\": \"12\", \"thirteen\": \"13\", \"fourteen\": \"14\", \"fifteen\": \"15\",\n",
        "    \"sixteen\": \"16\", \"seventeen\": \"17\", \"eighteen\": \"18\", \"nineteen\": \"19\",\n",
        "}\n",
        "\n",
        "# regex patterns for finding numbers\n",
        "re_tens = f'(?:{\"|\".join(re.escape(k) for k in tens_map_full.keys())})'\n",
        "re_numerical = f'(?:{\"|\".join(re.escape(k) for k in numerical_map.keys())})'\n",
        "re_ordinal = f'(?:{\"|\".join(re.escape(k) for k in ordinal_map.keys())})'\n",
        "re_numerical_text = f'\\\\b(?:{re_tens}?-?{re_numerical}|{re_tens})\\\\b(?!-)'\n",
        "re_ordinal_text = f'\\\\b{re_tens}?-?{re_ordinal}\\\\b(?!-)'\n",
        "\n",
        "# example\n",
        "print(re.findall(re_numerical_text, \"\"\"first, fourth and fifth, as well as twentieth twenty-first to twenty-three,\n",
        "        twenty-third and thirtyfourth and tenth three and thirtyfive and stwenty twenty sfirst\"\"\", flags=re.IGNORECASE))\n",
        "print(re.findall(re_ordinal_text, \"\"\"first, fourth and fifth, as well as twentieth twenty-first to twenty-three,\n",
        "        twenty-third and thirtyfourth and tenth three and thirtyfive and stwenty twenty sfirst\"\"\", flags=re.IGNORECASE))"
      ],
      "metadata": {
        "id": "jIOFsnG8bwq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RegEx for enumerations\n",
        "re_enum_number = \"(?:[0-9]{1,5})\" # Match number enumerations: 1 2 3 up to 99997 99998 99999\n",
        "re_enum_letter = \"(?:[A-Z]{1,3})\" # Match letter enumerations A B C up to ZZX ZZY ZZZ\n",
        "re_enum_letter_ci = \"(?:[A-Z]{1,3}|[a-z]{1,3})\" # also includes a b c up to zzx zzy zzz (but not mixed uppercase and lowercase!)\n",
        "re_enum_roman = \"(?:\\\\bM{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})\\\\b)\" # Match roman numerals (I-II-III)\n",
        "re_enum_roman_ci = f\"(?:{re_enum_roman}|{re_enum_roman.lower()})\" # Also match lowercase roman numerals (i-ii-iii)\n",
        "re_enum_mix = f\"(?:{re_enum_number}{re_enum_letter_ci}|{re_enum_letter_ci}{re_enum_number})\" ## Match mixed enumerations: 1a 1b 1c up to 99999zzx 99999zzy 99999zzz and a1 a2 a3 up to zzz99997 zzz99998 zzz99999, or with uppercase letters\n",
        "\n",
        "\n",
        "# RegEx for finding cross-reference codes, up to 6 levels, starting with the dividable enumerations and ending with any enumeration:\n",
        "# 1.1 1.2 1.3   1.1.1 1.1.2 1.1.3   etc.\n",
        "re_hier_codes = f\"\\\\b(?:(?:{re_enum_mix}|{re_enum_number}|{re_enum_letter})\\.\"+\"){1,5}\"+f\"(?:{re_enum_mix}|{re_enum_number}|{re_enum_roman}|{re_enum_letter})\\\\b\\.?\"\n",
        "\n",
        "# match all references to structural elements, such as article\n",
        "re_hier_cref = \"(?:\\\\b\"+\"|\".join([hier_labels[x]['re_translated'] for x in hier_labels])+\"\\\\b)\"\n",
        "\n",
        "# matches (sub)articles specifically\n",
        "re_hier_art = \"(?:\"+hier_labels[\"article\"]['re_translated']+\")\"\n",
        "re_hier_sub = \"(?:\"+hier_labels[\"sub-article\"]['re_translated']+\")\"\n",
        "\n",
        "# matches ordinal numbers in text format, such as first, seventh, twentyfifth, etc.\n",
        "re_hier_ordinal_num = \"(?:\\\\b(?:[0-9]+(?:st|nd|rd|th))\\\\b)\"\n",
        "re_hier_ordinal = f\"(?:{re_hier_ordinal_num}|{re_ordinal_text})\"\n",
        "\n",
        "# matches relative paths, e.g. \"this article\", \"current paragraph\"\n",
        "re_hier_relative = \"\\\\b(?:this|that|current)\\\\b\"\n",
        "#re_hier_all = \"\\\\b(?P<relative>this|current|all|every)\\\\b\"\n",
        "\n",
        "\n",
        "# matches all relevant ways to make conjunctions\n",
        "#re_hier_conj = \"(?:\\, |(?:\\,)? and | except(?: for)? | (?:up )?to(?: and including)? )\"\n",
        "re_hier_conj = \"(?:\\, |(?:\\,)? and | (?:up )?to(?: and including)? )\"\n",
        "\n",
        "# matches all possible identifiers and combinations at the start of a cross-reference\n",
        "#re_hier_id_start = f\"(?:(?P<id_rel>{re_hier_relative})|(?P<id_ord>{re_hier_ordinal}(?:{re_hier_conj}{re_hier_ordinal})*))\"\n",
        "re_hier_id_ord = f\"{re_hier_ordinal}(?:{re_hier_conj}{re_hier_ordinal})*\"\n",
        "\n",
        "# matches all possible identifiers and combinations at the end of a cross-reference\n",
        "re_hier_id_num = f\"{re_hier_codes}(?:{re_hier_conj}{re_hier_codes})*\"\n",
        "\n",
        "# ensures match starts at the beginning of a word (also not after a hyphen)\n",
        "re_start = \"(?<!\\\\-)\\\\b\"\n",
        "\n",
        "# Full regex patterns. looks for four distinct patterns:\n",
        "#re_hier_cref1 = f\"(?:the )?({re_hier_id_start}) (?P<sub1>{re_hier_sub})\"\n",
        "# 1. ordinal followed by subarticle\n",
        "re_hier_cref1 = f\"{re_start}(?P<id1_sub>{re_hier_id_ord}) (?P<label1_sub>{re_hier_sub})\"\n",
        "# 2. relative label followed by label\n",
        "re_hier_cref2 = f\"{re_start}(?P<rel2>{re_hier_relative}) (?P<label2>{re_hier_cref})\"\n",
        "# 3. label followed by code\n",
        "re_hier_cref3 = f\"{re_start}(?P<label3>{re_hier_cref}) (?P<id3>{re_hier_id_num})\"\n",
        "# 4. article code followed by ordinal+subarticle/subarticle + number\n",
        "re_hier_cref4 = f\"{re_start}(?P<label4_art>{re_hier_art}) (?P<id4_art>{re_hier_id_num})(?:\\,)? (?:(?P<id4_sub1>{re_hier_id_ord}) (?P<label4_sub1>{re_hier_sub})|(?P<label4_sub2>{re_hier_sub}) (?P<id4_sub2>{re_hier_id_num}))\"\n",
        "\n",
        "re_hier_cref = f\"({re_hier_cref4}|{re_hier_cref3}|{re_hier_cref2}|{re_hier_cref1})\"\n",
        "\n",
        "# example:\n",
        "# match = re.findall(re_hier_cref, \"the current paragraph, the 1st and 2nd subarticle, table 1 and 1.2.3.3 and 1.3. article 1.2. Article 4.53, sub-article 8. This is also true for this paragraph.\", flags=re.IGNORECASE)\n",
        "matches = re.finditer(re_hier_cref, \"The current section. First and second subarticle. table 1.2 and 1.2.3 and article 1.2. Article 4.6 to 4.8. Article 4.53, first sub-article. This paragraph.\", flags=re.IGNORECASE)\n",
        "[(m.start(), m.end(), m[0]) for m in matches]"
      ],
      "metadata": {
        "id": "aFwB5Na6foCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (before)"
      ],
      "metadata": {
        "id": "c4WBzZhIbd_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_sentences_cref_eval = [\n",
        "    \"First and second subarticle.\",\n",
        "    \"This paragraph.\",\n",
        "    \"Table 1.2 and 1.2.3, and article 1.2.\",\n",
        "    \"Article 4.43 to 4.45a.\",\n",
        "    \"Article 4.53, first sub-article.\"\n",
        "]"
      ],
      "metadata": {
        "id": "oK-nUDxHZ7Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_cref_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "qjbAooxEbfd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to pipeline"
      ],
      "metadata": {
        "id": "DYOFH_sAIV25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom pipeline component\n",
        "def crossref_recognizer(doc, regex_pattern):\n",
        "    # Find matches using re.finditer, which returns match objects\n",
        "    matches = re.finditer(regex_pattern, doc.text, flags=re.IGNORECASE)\n",
        "\n",
        "    # check each match for spans\n",
        "    custom_spans = []\n",
        "    for m in matches:\n",
        "        span = doc.char_span(m.start(), m.end())\n",
        "\n",
        "        # If no span found, try trimming whitespace or punctuation\n",
        "        if span is None:\n",
        "            trimmed_match = m.group(0).rstrip(' ').strip(',').strip('.')#.strip(' and')\n",
        "            trimmed_start = doc.text.find(trimmed_match)\n",
        "            trimmed_end = trimmed_start + len(trimmed_match)\n",
        "            span = doc.char_span(trimmed_start, trimmed_end)\n",
        "\n",
        "        # If span found, try assigning a label\n",
        "        if span is not None:\n",
        "            span.label_ = \"CROSS_REFERENCE\"\n",
        "            span._.score = 1\n",
        "            custom_spans.append(span)\n",
        "\n",
        "    # check for overlapping spans and remove the existing spans in case of overlap\n",
        "    existing_spans = list(doc.ents)\n",
        "    spans_to_remove = [] # gather spans to be removed in a separate list\n",
        "\n",
        "    for span in custom_spans:\n",
        "        for x in existing_spans:\n",
        "            if span.start <= x.end and span.end >= x.start:\n",
        "                # store overlapping existing spans to be removed in list to avoid duplicate removals\n",
        "                if x not in spans_to_remove:\n",
        "                    spans_to_remove.append(x)\n",
        "\n",
        "    # remove overlapping existing span at once to avoid conflicts with loop\n",
        "    for span_to_remove in spans_to_remove:\n",
        "            existing_spans.remove(span_to_remove)\n",
        "\n",
        "\n",
        "    # Add custom spans to the document's entities without overwriting existing ones\n",
        "    doc.ents = tuple(existing_spans) + tuple(custom_spans)\n",
        "    return doc"
      ],
      "metadata": {
        "id": "LPKeZ-OwQSOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"cref_recognizer\")#, assigns=[\"doc.ents\"])\n",
        "def entity_linker(doc):\n",
        "    doc = crossref_recognizer(doc, re_hier_cref)\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "try:\n",
        "    nlp.remove_pipe(\"cref_recognizer\") #remove old version when rerunning code\n",
        "except:\n",
        "    None\n",
        "# nlp.add_pipe(\"cref_recognizer\", after=\"gliner_spacy\")\n",
        "nlp.add_pipe(\"cref_recognizer\", last=True)"
      ],
      "metadata": {
        "id": "gsdEJbiIFQzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (after)"
      ],
      "metadata": {
        "id": "3kasOMX3I6Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_cref_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "t4FR1IBwb2b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantity recognition"
      ],
      "metadata": {
        "id": "cSa6CzpVOA9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare regex patterns"
      ],
      "metadata": {
        "id": "vmNj8wbGZI9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "    print(reg['text_translated'])\n",
        "    print(re.findall(r'(?<!\\w)(([+-]?\\d+(?:\\.\\d+)?(?:\\s*\\/\\s*\\d+)?(?:\\s*[°μΩ²³⁴⁵⁶23456]?\\s*[a-zA-Z²³⁴⁵⁶μΩ\\d]+(?:\\s*\\/\\s*[a-zA-Z²³⁴⁵⁶23456μΩ\\d]+)*))|(?:[0-9]+(?:\\.[0-9]+)? ?%))', reg['text_translated']))\n",
        "    print()"
      ],
      "metadata": {
        "id": "FaqhcTUGOFhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (before)"
      ],
      "metadata": {
        "id": "gKP9tibPq7X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_sentences_quant_eval = [\n",
        "    \"The heat radiation that can occur is larger than 2 kW/m2.\",\n",
        "    \"A temperature can occur that is higher than 90 ° C.\",\n",
        "    \"A sub-fire compartment with an internal cross-section larger than 0.015 m2, complies with fire class A2, determined according to NEN-EN 13501-1.\",\n",
        "    \"At most 5% of the total area ...\",\n",
        "]"
      ],
      "metadata": {
        "id": "u4NlBeEhrXNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_quant_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "SX8_E_oFrJ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to pipeline"
      ],
      "metadata": {
        "id": "ygACi4mqZLY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom pipeline component\n",
        "def quantity_recognizer(doc):\n",
        "    regex_pattern = r'(?<!\\w)(([+-]?\\d+(?:\\.\\d+)?(?:\\s?\\/\\s*\\d+)?\\s?%?(?:[°μΩ23456²³⁴⁵⁶]?\\s?[a-zA-ZμΩ\\d²³⁴⁵⁶]+(?:\\s?\\/\\s?[a-zA-ZμΩ\\d²³⁴⁵⁶]+)*))|(?:[0-9]+(?:\\.[0-9]+)?))'\n",
        "\n",
        "    # Find matches using re.finditer, which returns match objects\n",
        "    matches = re.finditer(regex_pattern, doc.text)#, flags=re.IGNORECASE) # This line was changed\n",
        "\n",
        "\n",
        "    updated_ents = list(doc.ents)\n",
        "    # check each match for spans\n",
        "    custom_spans = []\n",
        "    for m in matches:\n",
        "        if m[0] != '':\n",
        "            new_span = doc.char_span(m.start(), m.end())\n",
        "\n",
        "            # If no span found, try trimming whitespace or punctuation\n",
        "            if new_span is None:\n",
        "                trimmed_match = m.group(0).rstrip(' ').rstrip(',').rstrip('.')#.strip(' and')\n",
        "                trimmed_start = doc.text.find(trimmed_match)\n",
        "                trimmed_end = trimmed_start + len(trimmed_match)\n",
        "                new_span = doc.char_span(trimmed_start, trimmed_end)\n",
        "\n",
        "            # If still no span is found, try increasing the span length (needed for cases such as \"90 ° C.\", where spacy recognizes the period as part of the token)\n",
        "            if new_span is None:\n",
        "                new_span = doc.char_span(m.start(), m.end()+1)\n",
        "\n",
        "            # If span found, try assigning a label\n",
        "            if new_span is not None:\n",
        "                overlap_found = False\n",
        "                to_remove = []\n",
        "\n",
        "                for i, ent in enumerate(updated_ents):\n",
        "                    if (new_span.start < ent.end and new_span.end > ent.start):\n",
        "                        # Overlapping span found\n",
        "                        overlap_found = True\n",
        "                        if ent.label_ != \"CROSS_REFERENCE\" and ent.label_ != \"reference\" and ent.label_ != \"standard\" and ent.label_ != \"classification\":\n",
        "                            to_remove.append(i)\n",
        "                        break\n",
        "\n",
        "                # Remove old overlapping span if it's not a reference or classification\n",
        "                for i in sorted(to_remove, reverse=True):\n",
        "                    del updated_ents[i]\n",
        "\n",
        "                # Add new span if it's not overlapping a reference or classification\n",
        "                if not overlap_found or (overlap_found and not any((ent.label_ == \"CROSS_REFERENCE\" or  ent.label_ == \"reference\" or ent.label_ == \"standard\" or ent.label_ == \"classification\") and new_span.start < ent.end and new_span.end > ent.start for ent in updated_ents)):\n",
        "                    # Create span with label \"UNIT\" and assign a 100% score\n",
        "                    span = Span(doc, new_span.start, new_span.end, label=\"unit\")\n",
        "                    span._.score = 1.0\n",
        "                    updated_ents.append(span)\n",
        "\n",
        "    # Sort to maintain proper order\n",
        "    updated_ents = sorted(updated_ents, key=lambda x: (x.start, x.end))\n",
        "    doc.ents = updated_ents\n",
        "\n",
        "    return doc"
      ],
      "metadata": {
        "id": "0plZxdP8azPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"quantity_recognizer\")#, assigns=[\"doc.ents\"])\n",
        "def entity_linker(doc):\n",
        "    doc = quantity_recognizer(doc)\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "try:\n",
        "    nlp.remove_pipe(\"quantity_recognizer\") #remove old version when rerunning code\n",
        "except:\n",
        "    None\n",
        "# nlp.add_pipe(\"quantity_recognizer\", after=\"cref_recognizer\")\n",
        "nlp.add_pipe(\"quantity_recognizer\", last=True)"
      ],
      "metadata": {
        "id": "3BP8w_tWZRnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (after)"
      ],
      "metadata": {
        "id": "D0bqKz25sgBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_quant_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "kTzk6wv6shlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quality recognition**\n",
        "\n"
      ],
      "metadata": {
        "id": "L3rj8ODEPCfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "text = \"A cell unit located in an enclosed building space that is enclosed is a protected sub-fire compartment.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Find nouns with outgoing 'relcl' relation to a verb -- could also try using token.dep_ = \"quant\" or token.dep_ = \"attr\"?\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Check if token is a noun\n",
        "        for child in token.children:  # Iterate over its children\n",
        "            if child.dep_ == \"relcl\" and child.pos_ in [\"VERB\", \"AUX\"]:\n",
        "                # get span for new entity\n",
        "                print(f\"Noun: {token.text}, Relcl Verb: {child.text}\")\n"
      ],
      "metadata": {
        "id": "i4P5Tkxj1zue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom pipeline component\n",
        "def quality_recognizer(doc):\n",
        "    updated_ents = list(doc.ents)\n",
        "    # Find nouns with outgoing 'relcl' relation to a verb -- could also try using token.dep_ = \"quant\" or token.dep_ = \"attr\"?\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Check if token is a noun\n",
        "            for child in token.children:  # Iterate over its children\n",
        "                if child.dep_ == \"relcl\" and child.pos_ in [\"VERB\", \"AUX\"]:\n",
        "                    # print(f\"Noun: {token.text}, Relcl Verb: {child.text}\")\n",
        "                    # get span for new entity\n",
        "                    new_span = doc.char_span(child.idx, child.idx+len(child))\n",
        "\n",
        "                    # If span found, try assigning a label\n",
        "                    if new_span is not None:\n",
        "                        overlap_found = False\n",
        "                        to_remove = []\n",
        "\n",
        "                        for i, ent in enumerate(updated_ents):\n",
        "                            if (new_span.start < ent.end and new_span.end > ent.start):\n",
        "                                # Overlapping span found\n",
        "                                overlap_found = True\n",
        "                                to_remove.append(i)\n",
        "\n",
        "                        # Remove old overlapping span if it's not a reference or classification\n",
        "                        for i in sorted(to_remove, reverse=True):\n",
        "                            del updated_ents[i]\n",
        "\n",
        "                        # Add new span if it's not overlapping a reference or classification\n",
        "                        if not overlap_found:\n",
        "                            # Create span with label \"QUALITY\" and assign a 100% score\n",
        "                            span = Span(doc, new_span.start, new_span.end, label=\"QUALITY\")\n",
        "                            span._.score = 1.0\n",
        "                            updated_ents.append(span)\n",
        "\n",
        "    # Sort to maintain proper order\n",
        "    updated_ents = sorted(updated_ents, key=lambda x: (x.start, x.end))\n",
        "    doc.ents = updated_ents\n",
        "    return doc"
      ],
      "metadata": {
        "id": "gdsZsp8JM0GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "text = \"A cell unit located in a building space that is enclosed is a protected sub-fire compartment.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "# Process the text\n",
        "doc = quality_recognizer(doc)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "ZGiHLifHQVdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (before)"
      ],
      "metadata": {
        "id": "iaMMbHNba-np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_sentences_qual_eval = [\"A cell unit located in a building space that is enclosed is a protected sub-fire compartment.\"]"
      ],
      "metadata": {
        "id": "bXIN_ZZJbWel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_qual_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "dIK98wfKaQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add to pipeline"
      ],
      "metadata": {
        "id": "cITaBHw3MzrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"quality_recognizer\")#, assigns=[\"doc.ents\"])\n",
        "def entity_linker(doc):\n",
        "    doc = quality_recognizer(doc)\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "try:\n",
        "    nlp.remove_pipe(\"quality_recognizer\") #remove old version when rerunning code\n",
        "except:\n",
        "    None\n",
        "# nlp.add_pipe(\"quality_recognizer\", after=\"cref_recognizer\")\n",
        "nlp.add_pipe(\"quality_recognizer\", last=True)"
      ],
      "metadata": {
        "id": "hb5-GhgLM9Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation (after)"
      ],
      "metadata": {
        "id": "tGqvyoPRbCiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text in l_sentences_qual_eval:\n",
        "  doc = nlp(text)\n",
        "  display_ner_score(doc)"
      ],
      "metadata": {
        "id": "ERkmnyXTbB_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER pipeline Validation"
      ],
      "metadata": {
        "id": "jU6CXz-fZQfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(doc)\n",
        "    for span in doc.ents:\n",
        "        print(f'{span.text:25} --> {span.label_}')\n",
        "    print()"
      ],
      "metadata": {
        "id": "PvmwlBU3ZGRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "    # doc = nlp(text_to_num(reg['text_translated']))\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    display_ner_score(doc)"
      ],
      "metadata": {
        "id": "o3oHjYeqNAo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Prepare Knowledge Bases**\n",
        "\n",
        "Knowledge Bases are created per entity type, mostly using ontologies.\n",
        "\n",
        "*Mostly using labels only, but information like descriptions can be included as well*"
      ],
      "metadata": {
        "id": "V9mkxA3le-44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Not yet included:\n",
        "*  systems & devices\n",
        "*  quality data\n",
        "*  materials & layers\n",
        "*  construction permanence\n",
        "\n"
      ],
      "metadata": {
        "id": "467Xp_4pooTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for converting ontologies to knowledge bases"
      ],
      "metadata": {
        "id": "I_n2SchsWGpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ontology to knowledge base\n",
        "def ONT2KB(ontology_url, prefix, rdf_type=OWL.Class, splitter=\"#\"): # splitter is sometimes \"/\"\n",
        "    kb = {} # Initialize kb as an empty dictionary within the function\n",
        "    # Parse graph\n",
        "    g = Graph()\n",
        "    g.parse(ontology_url, format=\"turtle\")\n",
        "\n",
        "    # Find literals (either the only item or the english item)\n",
        "    def get_best_literal(literals):\n",
        "        if not literals:\n",
        "            return None\n",
        "        for lit in literals:\n",
        "            if lit.language == \"en\":\n",
        "                return str(lit)\n",
        "        return str(next(iter(literals)))  # return first one\n",
        "\n",
        "    # Check each class (a owl:class) in the ontology\n",
        "    for c in g.subjects(RDF.type, rdf_type):\n",
        "\n",
        "        # Get URI\n",
        "        if splitter in c: # Check if splitter (\"#\" or \"/\") is present before splitting\n",
        "            uri = prefix+\":\"+str(c.split(splitter)[-1])\n",
        "        else: # Handle cases where splitter is not present (e.g., use the full URI)\n",
        "            # uri = prefix+\":\"+str(c) splitter or any other appropriate logic\n",
        "            continue # skip element entirely if no URI is present\n",
        "\n",
        "        # Get label (name)\n",
        "        label = get_best_literal(list(g.objects(c, RDFS.label)))\n",
        "        if not label: # Create label from the URL fragment if no explicit label is present\n",
        "            # get URL fragment and split PascalCase string  into separate words\n",
        "            label = re.sub(r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])', ' ', str(c.split(\"#\")[1]))\n",
        "            # Also remove prefixes like \"Ifc\" from name\n",
        "            if label.lower().startswith(prefix.lower()):\n",
        "                label = label[len(prefix):]\n",
        "            # Also change underscores to spaces\n",
        "            label = label.replace(\"_\", \" \")\n",
        "            # Remove leading & trailing spaces\n",
        "            label = label.strip()\n",
        "\n",
        "        # Get description (if any)\n",
        "        comment = get_best_literal(list(g.objects(c, RDFS.comment)))\n",
        "\n",
        "        # Get domain (if any)\n",
        "        domain = [str(d) for d in g.objects(c, Namespace(\"http://schema.org/\").domainIncludes)]\n",
        "\n",
        "        # Get range (if any)\n",
        "        range = [str(r) for r in g.objects(c, RDFS.range)]\n",
        "\n",
        "        # Add info to knowledge base\n",
        "        kb[uri] = kb.get(uri, {}) # Get existing value or initialize as empty dict\n",
        "        if label: # Assign name (if present)\n",
        "            kb[uri][\"name\"] = label   # Assign name\n",
        "        if comment: # Assign description (if present)\n",
        "            kb[uri][\"description\"] = comment\n",
        "        if domain: # Assign domain (if present)\n",
        "            kb[uri][\"domain\"] = domain\n",
        "        if range: # Assign range (if present)\n",
        "            kb[uri][\"range\"] = range\n",
        "\n",
        "    return kb # Return the populated kb dictionary"
      ],
      "metadata": {
        "id": "NJIR93KJSuMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spatial elements"
      ],
      "metadata": {
        "id": "_NrlZ10wk2TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all spatial elements\n",
        "kb_spatial_bot = {\n",
        "    \"bot:Site\" : {\"descriptors\": [\"building site\", \"construction site\"]},\n",
        "    \"bot:Building\" : {\"descriptors\": [\"building\", \"construction\"]},\n",
        "    \"bot:Storey\" : {\"descriptors\": [\"storey\", \"building storey\", \"building story\"]},\n",
        "    \"bot:Space\" : {\"descriptors\": [\"building space\", \"space\"]},\n",
        "    \"bot:Element\" : {\"descriptors\": [\"building element\", \"built element\"]},\n",
        "}\n",
        "kb_spatial_custom = {\n",
        "    \"ex:ProtectedSubFireCompartment\" : {\"descriptors\": [\"protected sub-fire compartment\", \"protected subfire compartment\"]},\n",
        "    \"ex:SubFireCompartment\" : {\"descriptors\": [\"sub-fire compartment\", \"subfire compartment\"]},\n",
        "    \"ex:FireCompartment\" : {\"descriptors\": [\"fire compartment\"]},\n",
        "    \"ex:ExtraProtectedEscapeRoute\" : {\"descriptors\": [\"additionally protected escape route\", \"extra protected escape route\"]},\n",
        "    \"ex:ProtectedEscapeRoute\" : {\"descriptors\": [\"protected escape route\"]},\n",
        "    \"ex:EscapeRoute\" : {\"descriptors\": [\"escape route\"]},\n",
        "}\n",
        "kb_spatial = {**kb_spatial_bot, **kb_spatial_custom}"
      ],
      "metadata": {
        "id": "CwhdAXHPq_EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_spatial"
      ],
      "metadata": {
        "id": "VTYHROqrSxLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building elements\n",
        "\n",
        "Using both BEO and ifcOWL"
      ],
      "metadata": {
        "id": "bzXHzGrIk4rA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEO knowledge base will contain name, URI and description"
      ],
      "metadata": {
        "id": "uJxyWRQSDkNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load BEO ontology as knowledge base\n",
        "kb_BEO = ONT2KB(\"https://cramonell.github.io/beo/actual/ontology.ttl\", \"beo\")\n",
        "kb_BEO[\"beo:Door\"]"
      ],
      "metadata": {
        "id": "fon1KeTnTSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ifcOWL knowledge base will only contain names"
      ],
      "metadata": {
        "id": "hplwhcjTDspx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ifcOWL ontology as knowledge base\n",
        "# kb_IFC = ONT2KB(\"https://standards.buildingsmart.org/IFC/DEV/IFC4/ADD2_TC1/OWL/ontology.ttl\", \"ifc\")\n",
        "kb_IFC = ONT2KB(\"https://cramonell.github.io/ifc/ifcowl/IFC4X3_ADD2/actual/ontology.ttl\", \"ifc\")\n",
        "kb_IFC['ifc:IfcDoor']"
      ],
      "metadata": {
        "id": "W3tbJzuMdShR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Materials\n",
        "\n",
        "simple workaround for now"
      ],
      "metadata": {
        "id": "mG52gRqWPmMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kb_materials = {\n",
        "    \"Glass\": {\"descriptors\": [\"glass\", \"tempered glass\", \"laminated glass\"]},\n",
        "              #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Concrete\": {\"descriptors\": [\"concrete\", \"reinforced concrete\", \"precast concrete\"]},\n",
        "                 #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Steel\": {\"descriptors\": [\"steel\", \"structural steel\", \"stainless steel\"]},\n",
        "              #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Wood\": {\"descriptors\": [\"wood\", \"plywood\", \"timber\", \"laminated wood\"]},\n",
        "             #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Masonry\": {\"descriptors\": [\"masonry\", \"brick\", \"concrete blocks\", \"stone\"]},\n",
        "                #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Insulation\": {\"descriptors\": [\"insulation\", \"mineral wool\", \"EPS\", \"foam board\"]},\n",
        "                   #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Finishes\": {\"descriptors\": [\"finishes\", \"paint\", \"plaster\", \"tiles\"]},\n",
        "                 #\"pset\": \"pset:Identity\", \"property\": \"props:Category\"},\n",
        "    \"Composites\": {\"descriptors\": [\"composites\", \"fiberglass\"]},\n",
        "                   #\"pset\": \"pset:Identity\", \"prop\": \"props:Category\"},\n",
        "    \"Plastics\": {\"descriptors\": [\"plastic\", \"PVC\", \"polycarbonate\", \"acrylic\"]},\n",
        "                 #\"pset\": \"pset:Identity\", \"prop\": \"props:Category\"},\n",
        "}"
      ],
      "metadata": {
        "id": "XeLvQgaLf-YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Properties\n",
        "\n",
        "*Correct psets are not yet taken into account*"
      ],
      "metadata": {
        "id": "N1XahFgPb4Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom building properties\n",
        "kb_custom_buildingprops = {\n",
        "    'props:Usefunction' : {\"pset\": \"pset:Other\", \"descriptors\": [\"use function\", \"use functions\", \"usage function\", \"usage functions\", \"use area\", \"area of use\"]},\n",
        "}"
      ],
      "metadata": {
        "id": "MecyU2G3axG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dimensions kb\n",
        "kb_dimensions = {\n",
        "    \"props:Width\" : {\"descriptors\" : [\"width\", \"wide\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Length\": {\"descriptors\" : [\"length\", \"long\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Height\": {\"descriptors\" : [\"height\", \"high\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Depth\": {\"descriptors\" : [\"depth\", \"deep\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Thickness\": {\"descriptors\" : [\"thickness\", \"thick\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Perimeter\": {\"descriptors\": [\"perimeter\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Area\": {\"descriptors\": [\"area\"], \"pset\": \"pset:Dimensions\"},\n",
        "    \"props:Volume\": {\"descriptors\": [\"volume\"], \"pset\": \"pset:Dimensions\"},\n",
        "    # \"props:ComputationHeight\": {\"name\": \"computation height\", \"pset\": \"pset:Dimensions\"},\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "eMQsJQVKoa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PROPS ontology as knowledge base\n",
        "kb_PROPS = ONT2KB(\"https://raw.githubusercontent.com/maximelefrancois86/props/refs/heads/master/IFC4-output.ttl\", \"props\", rdf_type=OWL.DatatypeProperty, splitter=\"/\")\n",
        "kb_PROPS['props:width']"
      ],
      "metadata": {
        "id": "xkQGl14JhNzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_PROPS['props:nominalWidth']"
      ],
      "metadata": {
        "id": "S01543KPpUwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualities\n",
        "\n",
        "*Simplified for now*"
      ],
      "metadata": {
        "id": "NONERt-9Gxtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kb_qualities = {\n",
        "    (\"props:Isenclosed\", True): {\"pset\": \"pset:Other\", \"descriptors\": [\"enclosed\", \"closed\"]},\n",
        "    (\"props:Isenclosed\", False): {\"pset\": \"pset:Other\", \"descriptors\": [\"unenclosed\", \"open\"]},\n",
        "    (\"props:Isexternal\", True): {\"pset\": \"pset:Common\", \"descriptors\": [\"external\", \"externally\", \"exterior\"]},\n",
        "    (\"props:Isexternal\", False): {\"pset\": \"pset:Common\", \"descriptors\": [\"internal\", \"internally\", \"interior\"]},\n",
        "    (\"props:Loadbearing\", True): {\"pset\": \"pset:Common\", \"descriptors\": [\"load bearing\", \"load-bearing\", \"loadbearing\", \"structural\"]},\n",
        "    (\"props:Loadbearing\", False): {\"pset\": \"pset:Common\", \"descriptors\": [\"non load bearing\", \"non-load bearing\", \"nonstructural\", \"non-structural\"]},\n",
        "}\n",
        "\n",
        "kb_qualities.get((\"props:Isenclosed\", True))"
      ],
      "metadata": {
        "id": "Z7Gf4j1QGzZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifications\n",
        "\n",
        "*Will not work for all types of classifications!*"
      ],
      "metadata": {
        "id": "F98h7pjsajyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns classifications into numbers, so they are ordinal"
      ],
      "metadata": {
        "id": "bN4nN9wjFgyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fire classifications\n",
        "kb_classifications = {\n",
        "    'props:Fireclass' : {\"pset\": \"pset:Other\",\n",
        "                         \"descriptors\": [\"fire class\", \"fire classification\"],\n",
        "                         \"classes\": ('F', 'E', 'D', 'C', 'B', 'A2', 'A1')},\n",
        "    'props:Fireresistance' : {\"pset\": \"pset:Other\",\n",
        "                              \"descriptors\": [\"fire resistance\", \"fire resistant\", \"fire spread resistance\", \"fire resistance class\",\n",
        "                              \"resistance to fire penetration and fire spread\", \"resistance to fire penetration\", \"resistance to fire spread\",\n",
        "                              \"fire spread resistance class\", \"fire resistance classification\", \"fire spread resistance classification\"],\n",
        "                              \"classes\": ('15', '30', '60', '90', '120')}, # NEN 6068\n",
        "    'props:Smokeresistance': {\"pset\": \"pset:Other\",\n",
        "                              \"descriptors\": [\"smoke control class\", \"smoke resistance\", \"resistance to smoke passage\"],\n",
        "                              \"classes\": ('Ra', 'R200')}\n",
        "    }"
      ],
      "metadata": {
        "id": "KEbXVxaaatLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_classifications.get('props:Fireclass')"
      ],
      "metadata": {
        "id": "uQ3Sv-w0NPyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantities"
      ],
      "metadata": {
        "id": "rXHb74gw1I0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load unit vocabulary\n",
        "kb_unit = Graph()\n",
        "# kb_unit.parse(\"https://qudt.org/2.1/vocab/unit.ttl\", format=\"turtle\") # most recent version\n",
        "kb_unit.parse(\"https://qudt.org/vocab/unit/\", format=\"turtle\") # using this version instead, which works better with quantity & unit normalization"
      ],
      "metadata": {
        "id": "uICDIMEw1IWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Entity Linking** (rule-based)\n",
        "\n"
      ],
      "metadata": {
        "id": "8gG56JtesN1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current script works mostly using rule-based approaches, but implementing statistical or neural approaches as well is recommended"
      ],
      "metadata": {
        "id": "ZdTb6RZWtMLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare subfunctions"
      ],
      "metadata": {
        "id": "KuOtDgjAwx3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find by keyword\n",
        "\n",
        "*Might return false positives for short keywords*"
      ],
      "metadata": {
        "id": "H_jFsnPFYn73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return entries where the keyword is found in a specific item of a database\n",
        "def find_by_keyword(kb, keyword, key, exact=False, list=False, plurality=True):\n",
        "    # Prepare dictionary for results\n",
        "    results = {}\n",
        "    # Match keywords\n",
        "\n",
        "    for curie, data in kb.items():\n",
        "        if list: # check items in a list\n",
        "            if exact and any(keyword.lower() == value.lower() for value in data.get(key, [])):\n",
        "                results[curie] = data\n",
        "            elif not exact and any(keyword.lower() in value.lower() for value in data.get(key, [])):\n",
        "                results[curie] = data\n",
        "        else: # check an individual item\n",
        "            if exact and data.get(key, \"\").lower() == keyword.lower():\n",
        "                results[curie] = data\n",
        "            elif not exact and keyword.lower() in data.get(key, \"\").lower():\n",
        "                results[curie] = data\n",
        "    # if no match is found and exact match is enabled, try again while removing s from the end (which might indicate plurality)\n",
        "    if not results and exact == True and plurality == True:\n",
        "        results = find_by_keyword(kb, keyword.rstrip('s'), key, exact=exact, list=True, plurality=False)\n",
        "    return results\n",
        "find_by_keyword(kb_BEO, \"sun\", \"description\")"
      ],
      "metadata": {
        "id": "7T6-_u3zVxmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_by_keyword(kb_spatial_bot, \"building spaces\", \"descriptors\", list=True, exact=True)"
      ],
      "metadata": {
        "id": "SboorOfc8RQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Material & quality match\n",
        "\n",
        "*Simple workaround for now*"
      ],
      "metadata": {
        "id": "Fx92s_zhPgy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_match_quality(text):\n",
        "    match = find_by_keyword(kb_qualities, text, \"descriptors\", list=True, exact=True)\n",
        "    if match:\n",
        "        return {\"pset\" : list(match.values())[0].get('pset'),\n",
        "                \"property\": list(match.keys())[0][0]}\n",
        "    else:\n",
        "        return None\n",
        "cand_match_quality(\"unenclosed\")"
      ],
      "metadata": {
        "id": "YOw1bD8qLw9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_match_material(text):\n",
        "    match = find_by_keyword(kb_materials, text, \"descriptors\", list=True, exact=True)\n",
        "    if match:\n",
        "        return {\"material_pset\" : \"pset:Identity\",\n",
        "                \"material_property\": \"props:Category\",\n",
        "                \"material_category\": list(match.keys())[0]}\n",
        "    else:\n",
        "        return None\n",
        "cand_match_material(\"PVC\")"
      ],
      "metadata": {
        "id": "SNAfqgxQ1kmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjective match\n",
        "\n",
        "Tries to match qualitities and quantities from adjectives (nested entities)"
      ],
      "metadata": {
        "id": "do6lbCOvA5T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_match_adjective(adjective):\n",
        "    if adjective == \"\":\n",
        "        return {}\n",
        "    quality = cand_match_quality(adjective)\n",
        "    if quality:\n",
        "        return quality\n",
        "    else:\n",
        "        material = cand_match_material(adjective)\n",
        "        if material:\n",
        "            return material\n",
        "        else:\n",
        "            return {}"
      ],
      "metadata": {
        "id": "iX0XvuRTMc2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial matching\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4nJlCul1P7sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_match(kb, text, key, list=False, exact=True):\n",
        "\n",
        "    # First, try removing tokens on the left\n",
        "    candidate = text.split(\" \") # split text by spaces\n",
        "    trimmed = \"\"\n",
        "    while len(candidate) > 0: # keep trying until down to the last word\n",
        "        # Try to find candidate match in names of the knowledge base\n",
        "        cand_match = find_by_keyword(kb, \" \".join(candidate), key, list=list, exact=exact)\n",
        "        if cand_match:  # return first match as linked entity + trimmed\n",
        "            return next(iter(cand_match)), trimmed.strip(\" \")\n",
        "        trimmed += candidate[0] + \" \"\n",
        "        candidate = candidate[1:]\n",
        "\n",
        "    # Then, try removing tokens on the right\n",
        "    candidate = text.split(\" \") # split text by spaces\n",
        "    trimmed = \"\"\n",
        "    while len(candidate) > 0: # keep trying until down to the last word\n",
        "        # Try to find candidate match in names of the knowledge base\n",
        "        cand_match = find_by_keyword(kb, \" \".join(candidate), key, list=list, exact=exact)\n",
        "        if cand_match:  # return first match as linked entity + trimmed\n",
        "            return next(iter(cand_match)), trimmed.strip(\" \")\n",
        "        trimmed = candidate[-1] + \" \" + trimmed\n",
        "        candidate = candidate[:-1]\n",
        "\n",
        "    # Return empty if no matches are found\n",
        "    return None, \"\"\n",
        "\n",
        "match = partial_match(kb_BEO, \"external doors\", \"name\")\n",
        "{\"element\": match[0], **cand_match_adjective(match[1])}"
      ],
      "metadata": {
        "id": "dlnT0D84eStt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification match\n",
        "\n",
        "*Tries to make the classification ordinal*"
      ],
      "metadata": {
        "id": "qQavb4LFb71c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kb_classifications['props:Fireresistance']['classes']"
      ],
      "metadata": {
        "id": "aHyQKPP28WqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_match_classification(text):\n",
        "    # first, find if there is a name and class from the classifications kb\n",
        "    class_name, class_code = partial_match(kb_classifications, text, \"descriptors\", list=True)\n",
        "    if not class_name:\n",
        "        return None\n",
        "    class_prop = kb_classifications[class_name]\n",
        "\n",
        "    # special case for fire resistance (which is not ordinal but in minutes)\n",
        "    if class_name == 'props:Fireresistance':\n",
        "        return {'pset': 'pset:Other',\n",
        "                'property': 'props:Fireresistance',\n",
        "                'quantity': int([c for c in kb_classifications['props:Fireresistance']['classes'] if c in class_code][0])}\n",
        "\n",
        "    # try to make the classification ordinal, try to match class or part of class to property\n",
        "    if class_prop.get('classes'):\n",
        "        for i, clss in enumerate(class_prop['classes']):\n",
        "            if clss.lower() in class_code.lower(): # case-insensitive\n",
        "                return {'pset': 'pset:Other',\n",
        "                        'property': class_name,\n",
        "                        'classification': clss,\n",
        "                        'compliant_classes': class_prop['classes'][i:]} # store all classes that are also met for this specific class\n",
        "        # if no ordinal match found, return normal matches\n",
        "        if class_code:\n",
        "            return {'pset': 'pset:Other',\n",
        "                    'property': class_name,\n",
        "                    'classification': class_code}\n",
        "    # try to match without classification value\n",
        "    if class_prop:\n",
        "        return {'pset': 'pset:Other',\n",
        "                'property': class_name}\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(cand_match_classification(\"fire class D\"))\n",
        "print(cand_match_classification(\"fire resistance of 90 minutes\"))\n",
        "print(cand_match_classification(\"fire resistant for 30 minutes\"))"
      ],
      "metadata": {
        "id": "0DidU5Q2PjSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Props match\n",
        "\n",
        "*PROPS returns many false positives when checking for description, so this has been disabled*"
      ],
      "metadata": {
        "id": "Dz4pwlQZkYYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert camelCase to PascalCase (which is used by IfcOpenShell for classes)\n",
        "def camel2pascal(text):\n",
        "    # try splitting prefix from URI (if prefix is present)\n",
        "    try:\n",
        "        prefix = text.split(':')[0]\n",
        "        clss = text.split(':')[1]\n",
        "        return ':'.join([prefix, clss[0].upper()+clss[1:]])\n",
        "    except: # without prefix\n",
        "        return text[0].upper()+text[1:]\n",
        "\n",
        "def cand_match_props(text):\n",
        "    # First, try finding the name as one of the manually defined properties\n",
        "    property = find_by_keyword(kb_custom_buildingprops, text, \"descriptors\", list=True, exact=True)\n",
        "    if not property:\n",
        "        property = find_by_keyword(kb_dimensions, text, \"descriptors\", list=True, exact=True)\n",
        "\n",
        "    # If custom KB match is found, return it\n",
        "    if property:\n",
        "        key = next(iter(property.keys()), None)\n",
        "        values = list(property.values())[0]\n",
        "        return {'property': key,\n",
        "                'pset': values.get('pset')}\n",
        "\n",
        "    # Then, try matching as classification\n",
        "    classification = cand_match_classification(text)\n",
        "    if classification:\n",
        "        return classification\n",
        "\n",
        "    # Then, try finding the text as name of a property\n",
        "    property = find_by_keyword(kb_PROPS, text.replace(' ', ''), \"name\", exact=True)\n",
        "    # # Then, try finding it in a description\n",
        "    # if not property:\n",
        "    #     property = find_by_keyword(kb_PROPS, text, \"description\")\n",
        "\n",
        "    # If no match is found: return generic props name. Also for multiple matches?\n",
        "    if len(property) != 1:\n",
        "        return {'property': 'props:'+text.title().replace(' ','')}\n",
        "                #'pset': None}\n",
        "\n",
        "    # If PROPS match is found, return it\n",
        "    key = camel2pascal(next(iter(property.keys()), None))\n",
        "    values = list(property.values())[0]\n",
        "    return {'property': key}\n",
        "            # 'pset': values.get('range')[0],\n",
        "            # 'pset': values.get('domain')[0],}\n",
        "\n",
        "cand_match_props(\"wide\")"
      ],
      "metadata": {
        "id": "H_RKva92kYNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantity match\n",
        "\n",
        "*Might return incorrect standardized units for some elements, but standardized values should be correct*"
      ],
      "metadata": {
        "id": "H5YF2LcIGw-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define namespaces\n",
        "QUDT = Namespace(\"http://qudt.org/schema/qudt/\")\n",
        "UNIT = Namespace(\"http://qudt.org/vocab/unit/\")\n",
        "\n",
        "# # To deal with floating point errors:\n",
        "# from decimal import Decimal, getcontext\n",
        "# getcontext().prec = 4  # Set precision to 4 decimal places\n",
        "\n",
        "# Function for finding base unit (after conversion)\n",
        "def get_base_unit(qudt_unit) -> URIRef:\n",
        "    # Special case for grams, where kilograms is the base unit\n",
        "    if \"GM\" in str(qudt_unit):\n",
        "        return UNIT[\"KiloGM\"]\n",
        "    # Option A: Try simple scaling unit (e.g., MilliM)\n",
        "    scaled_from = kb_unit.value(subject=URIRef(qudt_unit), predicate=QUDT.scalingOf)\n",
        "    if scaled_from:\n",
        "        return scaled_from\n",
        "\n",
        "    # Option B: Handle complex factor units\n",
        "    base_units = []\n",
        "    for factor_unit in kb_unit.objects(subject=URIRef(qudt_unit), predicate=QUDT.hasFactorUnit):\n",
        "        base = kb_unit.value(subject=factor_unit, predicate=QUDT.hasUnit)\n",
        "        exponent = kb_unit.value(subject=factor_unit, predicate=QUDT.exponent)\n",
        "\n",
        "        # Recursively find the base of this unit\n",
        "        base_of_base = get_base_unit(base)\n",
        "        base_units.append((int(exponent), base_of_base))\n",
        "\n",
        "    # Construct base unit name (e.g., unit:N-M2)\n",
        "    # This assumes the original vocabulary uses this naming convention\n",
        "    numerators = []\n",
        "    denominators = []\n",
        "\n",
        "    for exponent, base_unit in base_units:\n",
        "        base_name = str(base_unit).split(\"/\")[-1]  # extract local name\n",
        "        if exponent > 0:\n",
        "            part = f\"{base_name}\" if exponent == 1 else f\"{base_name}{exponent}\"\n",
        "            numerators.append(part)\n",
        "        elif exponent < 0:\n",
        "            part = f\"{base_name}\" if exponent == -1 else f\"{base_name}{-exponent}\"\n",
        "            denominators.append(part)\n",
        "\n",
        "    base_name = \"-\".join(numerators)\n",
        "    if denominators:\n",
        "        base_name += \"-PER-\" + \"-\".join(denominators)\n",
        "    return base_name\n",
        "\n",
        "\n",
        "def standardize_quantity(value, unit):\n",
        "    # Making sure metres is always specified as \"m\" instead of \"M\" (while not modifying M for mega)\n",
        "    unit = re.sub(r'M(?![^\\dΩ])', 'm', unit)\n",
        "    # Different unit representations to try:\n",
        "    unit_1 = unit.replace(\"2\", \"Â²\").replace(\"3\", \"Â³\").replace(\"4\", \"Â⁴\").replace(\"5\", \"Â⁵\").replace(\"6\", \"Â⁶\")\n",
        "    unit_2 = unit.replace(\"2\", \"²\").replace(\"3\", \"³\").replace(\"4\", \"⁴\").replace(\"5\", \"⁵\").replace(\"6\", \"⁶\")\n",
        "    unit_3 = unit.replace(\"²\", \"2\").replace(\"³\", \"3\").replace(\"⁴\", \"4\").replace(\"⁵\", \"5\").replace(\"⁶\", \"6\")\n",
        "\n",
        "    # Query the units. Alternatively, case-insensitive version:  FILTER(LCASE(?symbol) = LCASE(\"{unit_X}\"))\n",
        "    query_units = f'''\n",
        "    PREFIX qudt: <{QUDT}>\n",
        "    PREFIX rdfs: <{RDFS}>\n",
        "\n",
        "    SELECT ?unitname ?factor ?kind\n",
        "    WHERE {{\n",
        "        {{\n",
        "            ?unitname qudt:symbol ?symbol .\n",
        "            FILTER( ?symbol = \"{unit_1}\" )\n",
        "        }}\n",
        "        UNION {{\n",
        "            ?unitname qudt:symbol ?symbol .\n",
        "            FILTER( ?symbol = \"{unit_2}\" )\n",
        "        }}\n",
        "        UNION {{\n",
        "            ?unitname qudt:uCumCode ?code .\n",
        "            FILTER( STR(?code) = \"{unit_3}\" )\n",
        "        }}'''\n",
        "    # If unit has exponents in the denominator, replace those with a different notation for uCumCode\n",
        "    if \"/\" in unit:\n",
        "        unit_4 = unit_3.split('/')[0]+'.'+unit_3.split('/')[1].replace(\"2\", \"-2\").replace(\"3\", \"-3\").replace(\"4\", \"-4\").replace(\"5\", \"-5\").replace(\"6\", \"-6\")\n",
        "        query_units += f'''        UNION {{\n",
        "            ?unitname qudt:uCumCode ?code .\n",
        "            FILTER( STR(?code) = \"{unit_4}\" )\n",
        "        }}'''\n",
        "    # Also try to find text labels if the unit is purely text-based\n",
        "    if re.match('^[a-zA-Z]{3,}$', unit):\n",
        "        unit_5 = unit.strip(' ')\n",
        "        unit_6 = unit.strip(' ').rstrip('s')\n",
        "        query_units += f'''\n",
        "        UNION {{\n",
        "            ?unitname rdfs:label \"?label\" .\n",
        "            FILTER( LCASE(STR(?label)) = LCASE(\"{unit_5}\") )\n",
        "        }}\n",
        "        UNION {{\n",
        "            ?unitname rdfs:label ?label .\n",
        "            FILTER( LCASE(STR(?label)) = LCASE(\"{unit_6}\") )\n",
        "        }}'''\n",
        "    query_units += '\\n}' # close brackets in query\n",
        "\n",
        "    # Perform query\n",
        "    results = kb_unit.query(query_units)\n",
        "\n",
        "    if not results:\n",
        "        print(\"error converting\", value, unit)\n",
        "        return value, None\n",
        "    for row in results:\n",
        "        if not row.unitname:\n",
        "            None\n",
        "        else:\n",
        "            qudt_unit = str(row.unitname)\n",
        "\n",
        "\n",
        "    # Check conversion multiplier\n",
        "    factor = kb_unit.value(subject=URIRef(qudt_unit), predicate=QUDT.conversionMultiplier)\n",
        "    # Skip base unit retrieval if factor = 1 (assumes base unit for those cases)\n",
        "    if factor and float(factor) == 1.0:\n",
        "        return float(value), qudt_unit.replace(UNIT, 'unit:')\n",
        "    # Special case for minutes, which are used for fire resistance classes and should not be standardized?\n",
        "    elif unit == \"minutes\":\n",
        "        return value, unit\n",
        "    elif not factor:\n",
        "        return float(value), unit\n",
        "    else:\n",
        "        return float(factor)*float(value), \"unit:\"+get_base_unit(qudt_unit).replace(UNIT, '')\n",
        "\n",
        "\n",
        "# Example use:\n",
        "print(standardize_quantity(90, \"kW/m2\"))\n",
        "print(standardize_quantity(\"200\", \"MJ/m2\"))\n",
        "print(standardize_quantity(\"200\", \"MJ/M2\"))\n",
        "print(standardize_quantity(\"90\", \"cm\"))\n",
        "print(standardize_quantity(\"3\", \"meters\"))\n",
        "print(standardize_quantity(\"3\", \"centimeter\"))\n",
        "print(standardize_quantity(\"90\", \"g\"))\n",
        "print(standardize_quantity(\"90\", \"kW/m2\"))\n",
        "print(standardize_quantity(\"90\", \"minutes\"))\n",
        "print(standardize_quantity(\"5\", \"kN/m2\"))\n",
        "print(standardize_quantity(\"90\", \"°C\"))"
      ],
      "metadata": {
        "id": "lMt05GvklMsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"*minutes*\" *have been left out of the equation since they will always refer to a classification in case of fire safety and not a quantity.*\n",
        "\n",
        "*Even if this is true, \"90 minutes\" should return unit SEC but returns MIN while also modifying value so this should be fixed.*"
      ],
      "metadata": {
        "id": "1JHIYDwEAi-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_match_quantity(text):\n",
        "    # Remove spaces & thousands separators first\n",
        "    quantity = text.replace(\",\", \"\").strip(' ')\n",
        "    # Check if quantity is unitless\n",
        "    unitless = re.match(r'^-?\\d+(?:\\.\\d+)?$', quantity)\n",
        "    if unitless:\n",
        "        return {'quantity': unitless[0]}\n",
        "    # Check if quantity is a percentage and convert to number if true\n",
        "    percentage = re.match(r'^(-?\\d+(?:\\.\\d+)?) ?%', quantity)\n",
        "    if percentage:\n",
        "        return {'quantity': float(percentage[1])*0.01, 'unit': 'unit:PERCENT'}\n",
        "    # Find value and unit part of text\n",
        "    match = re.match(r\"(-?\\d+(?:\\.\\d+)?) ?((?:° )?\\S+)\", text) # Also addresses degree formatting issues\n",
        "    try:\n",
        "        value = match[1]\n",
        "        unit = match[2].rstrip('.') #remove periods at the end, which happens when spaCy doesn't identify the entity correctly\n",
        "        unit = unit.replace ('° ', '°') # Also addresses degree formatting issues\n",
        "        # special case for minutes (which is a fire class):\n",
        "        if \"minutes\" in unit:\n",
        "            fr_classes = kb_classifications['props:Fireresistance']['classes']\n",
        "            return {'pset': 'pset:Other',\n",
        "                    'property': 'props:Fireresistance',\n",
        "                    'quantity': int(value)\n",
        "                    # 'classification': f'\"{quantity}\"^^xsd:string',\n",
        "                    # 'compliant_classes': fr_classes[fr_classes.index(str(value)):]\n",
        "                    }\n",
        "        # For other cases:\n",
        "        result = standardize_quantity(value, unit)\n",
        "        if result:\n",
        "            v, u = result\n",
        "            # return {'quantity': v, 'unit': 'unit:'+u.split('/')[-1]} if u else {'quantity': v, 'unit': unit}\n",
        "            return {'quantity': v, 'unit': u} if u else {'quantity': v, 'unit': unit}\n",
        "        else:\n",
        "            return {'quantity': value, 'unit': unit}\n",
        "    except:\n",
        "        try: # Try to see if value was found without unit\n",
        "            return {'quantity': value}\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "print(cand_match_quantity('3500 mm'))\n",
        "print(cand_match_quantity('60 minutes'))\n",
        "print(cand_match_quantity(\"90 minutes\"))\n",
        "print(cand_match_quantity('90 ° C'))\n",
        "print(cand_match_quantity(\"200 MJ/m2\"))\n",
        "print(cand_match_quantity(\"90 cm\"))\n",
        "print(cand_match_quantity(\"3 meters\"))\n",
        "print(cand_match_quantity(\"90 g\"))\n",
        "print(cand_match_quantity(\"90 kW/m2\"))"
      ],
      "metadata": {
        "id": "pypFLUXk4LJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-reference linking\n",
        "\n",
        "*Basic implementation for now, will have to be improved for actual deployment*\n"
      ],
      "metadata": {
        "id": "lXJunvhsmC3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Find the parts of the text mentioning crefs & store it\n",
        "2.   Replace the references in the text with placeholders, and store reference text separately\n",
        "3.   Find the corresponding references for each reference text & store this with the reference text"
      ],
      "metadata": {
        "id": "eF6_SqmyGM7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace ordinal words with numbers\n",
        "def text_to_num(text):\n",
        "    # Regex pattern to first replace all orders of 10 (except for 10 itself)\n",
        "    tens_pattern_full = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in tens_map_full.keys()) + r')(?![a-z\\-])', re.IGNORECASE)\n",
        "    tens_pattern_part = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in tens_map_part.keys()) + r')(?=[a-z\\-])', re.IGNORECASE)\n",
        "\n",
        "    text = tens_pattern_full.sub(lambda match: tens_map_full[match.group(0).lower()], text)\n",
        "    text = tens_pattern_part.sub(lambda match: tens_map_part[match.group(0).lower()], text)\n",
        "\n",
        "    # Regex patterns to match ordinal strings\n",
        "    ordinal_pattern_full = re.compile(r'(?:\\b' + '|'.join(re.escape(k) for k in ordinal_map.keys()) + r')\\b', re.IGNORECASE)\n",
        "    ordinal_pattern_part = re.compile(r'(?:(?<=\\b[0-9])' + '|'.join(re.escape(k) for k in ordinal_map.keys()) + r')\\b', re.IGNORECASE)\n",
        "    text = ordinal_pattern_full.sub(lambda match: ordinal_map[match.group(0).lower()], text)\n",
        "    text = ordinal_pattern_part.sub(lambda match: ordinal_map[match.group(0).lower()], text)\n",
        "\n",
        "    # Regex patterns to match numerical strings\n",
        "    numerical_pattern_full = re.compile(r'(?:\\b' + '|'.join(re.escape(k) for k in numerical_map.keys()) + r')\\b', re.IGNORECASE)\n",
        "    numerical_pattern_part = re.compile(r'(?<=\\b[0-9])(?:' + '|'.join(re.escape(k) for k in numerical_map.keys()) + r')\\b', re.IGNORECASE)\n",
        "    text = numerical_pattern_full.sub(lambda match: numerical_map[match.group(0).lower()], text)\n",
        "    text = numerical_pattern_part.sub(lambda match: numerical_map[match.group(0).lower()], text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text = \"first, fourth and fifth, as well as twentieth twenty-first to twenty-three, twenty-third and thirtyfourth and tenth three and thirtyfive and stwenty twenty sfirst\"\n",
        "converted_text = text_to_num(text)\n",
        "print(text)\n",
        "print(converted_text)"
      ],
      "metadata": {
        "id": "dgweudfgsJc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_with_codes(text, mapping):\n",
        "    for item in mapping.values():\n",
        "        pattern = f'\\\\b{item[\"re_translated\"]}\\\\b'\n",
        "        code = item[\"code\"]\n",
        "        text = re.sub(pattern, code, text.replace(\"-\", \"\").replace(\"the \",\"\"), flags=re.IGNORECASE)  # Case-insensitive replacement\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "for text in [\"This chapter should be read along with the following sections.\",\n",
        "             \"ARTICLES 4.43 and 4.45a, first and second sub-articles, subarticle 1\"]:\n",
        "    print(text_to_num(replace_with_codes(text, hier_labels)))"
      ],
      "metadata": {
        "id": "5QrOWMKO6zzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cref_codes(cref_code, cref_text, hier_codes):\n",
        "    # replace spaces with underscores & split if there are multiple\n",
        "    l_cref_codes = cref_code.split('+')\n",
        "    for cref in l_cref_codes:\n",
        "        # check if there is a range present\n",
        "        if \"--\" in cref:\n",
        "            l_cref_codes.remove(cref)\n",
        "            cref_start, cref_end = cref.split('--')\n",
        "            print(\"---->\", cref_text)\n",
        "            #print(\"---->\", cref_start, cref_end)\n",
        "            in_range = False\n",
        "            # add references in range to dataset (assumes that list is sorted)\n",
        "            for reg in hier_codes:\n",
        "                if reg == cref_start:\n",
        "                    l_cref_codes = [reg]\n",
        "                    in_range = True\n",
        "                elif reg == cref_end:\n",
        "                    l_cref_codes.append(reg)\n",
        "                    in_range = False\n",
        "                    break\n",
        "                elif in_range:\n",
        "                    l_cref_codes.append(reg)\n",
        "    if len(l_cref_codes) == 1:\n",
        "        return l_cref_codes[0]\n",
        "    elif len(l_cref_codes) > len(hier_codes)/2 or len(l_cref_codes) > 60: #probably an error\n",
        "        print(\"error converting:\", cref_text)\n",
        "        return None\n",
        "    else:\n",
        "        return l_cref_codes"
      ],
      "metadata": {
        "id": "jokI0hWZyG-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code below is unfinished:*"
      ],
      "metadata": {
        "id": "2jwK20eVwu90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cref_codes(text, reg_code):\n",
        "    # remove unimportant words\n",
        "    text = text.lstrip('the').lstrip('The')\n",
        "\n",
        "    # pattern matching for for type 4 (article code followed by ordinal+subarticle/subarticle + number)\n",
        "    # (label4_art, id4_art, id4_sub1, label4_sub1, label4_sub2, id4_sub2)\n",
        "    # (should be checked first since it overlaps with the others!)\n",
        "    # cref_match = re.sub(re_hier_cref, \"\\g<label4_art>\\g<id4_art>, \\g<label4_sub1>\\g<label4_sub2> \\g<id4_sub1>\\g<id4_sub2>\", text, flags=re.IGNORECASE)\n",
        "    re_match = re.match(re_hier_cref4, text)\n",
        "    if re_match:\n",
        "        # print(4, re_match[0])\n",
        "        print(f'WIP! cref not converted: \"{re_match[0]}\"')\n",
        "    # #re.match()\n",
        "    # #cref_match = re.sub(re_hier_cref, \"\\g<label4_art>\\g<id4_art>, \\g<label4_sub1>\\g<label4_sub2> \\g<id4_sub1>\\g<id4_sub2>\", text, flags=re.IGNORECASE)\n",
        "    #if cref_match != \",  \":\n",
        "        # print(\"type 4\", text)\n",
        "        # cref_match = re_match['label3']\n",
        "        # cref_codes = replace_with_codes(text_to_num(cref_match), hier_labels)\n",
        "        # print(cref_codes)\n",
        "        # return cref_codes\n",
        "\n",
        "        # # replace \"SUB X, Y and Z\" with \"SUB_X+SUB_Y+SUB_Z\"\n",
        "        # cref_codes = re.sub(\"[ ]?(?:(?:\\,(?:and )?)|and) \" f\"+{base_URI}SUB\", cref_codes, flags=re.IGNORECASE)\n",
        "        # # replace ranges with \"--\"\n",
        "        # cref_codes = re.sub(\" ?(?:up )?to(?: and including)? \", f\"--{base_URI}SUB\", cref_codes, flags=re.IGNORECASE)\n",
        "        # # add base URI and remove spaces\n",
        "        # cref_codes = base_URI + cref_codes.replace(\" \", \"\")\n",
        "        # print(cref_codes)\n",
        "        #cref_codes = _find_cref_codes(cref_codes)\n",
        "        #return cref_codes\n",
        "\n",
        "    # pattern matching for for type 1 (ordinal followed by subarticle)\n",
        "    re_match = re.match(re_hier_cref1, text)\n",
        "    if re_match:\n",
        "        # print(1, re_match[0])\n",
        "        cref_match = re_match['label1_sub'] + ' '+re_match['id1_sub']\n",
        "        # print(cref_match)\n",
        "        #print(cref_match)\n",
        "    #cref_match = re.sub(re_hier_cref, \"\\g<label1_sub> \\g<id1_sub>\", text, flags=re.IGNORECASE)\n",
        "    #if cref_match != \" \":\n",
        "        #print(cref_match)\n",
        "        # replace hier labels with code (e.g. \"subarticle\" -> \"SUB\")\n",
        "        cref_codes = replace_with_codes(text_to_num(cref_match), hier_labels)\n",
        "        # replace ranked numbers with regular numbers\n",
        "        cref_codes = re.sub(\"([0-9]+)(?:st|nd|rd|th)?\", \"\\g<1>\", cref_codes, flags=re.IGNORECASE)\n",
        "        # get base URI to search for\n",
        "        base_URI = reg_code.split('SUB')[0]\n",
        "        # make regex pattern with base URI\n",
        "        #cref_codes = f\"{base_URI}\"\n",
        "        # replace \"SUB X, Y and Z\" with \"SUB_X+SUB_Y+SUB_Z\"\n",
        "        cref_codes = re.sub(\"[ ]?(?:(?:\\,(?:and )?)|and) \", f\"+{base_URI}SUB\", cref_codes, flags=re.IGNORECASE)\n",
        "        # replace ranges with \"--\"\n",
        "        cref_codes = re.sub(\" ?(?:up )?to(?: and including)? \", f\"--{base_URI}SUB\", cref_codes, flags=re.IGNORECASE)\n",
        "        # add base URI and remove spaces\n",
        "        cref_codes = base_URI + cref_codes.replace(\" \", \"\")\n",
        "        # print(cref_codes)\n",
        "        cref_codes = find_cref_codes(cref_codes, text, hier_codes)\n",
        "        return cref_codes\n",
        "\n",
        "    re_match = re.match(re_hier_cref2, text)\n",
        "    if re_match:\n",
        "    # if cref_match != \" \":\n",
        "        # print(9, re_match)\n",
        "        # skip coreference resolution for now\n",
        "        if \"that\" in re_match:\n",
        "            print(f'coreference resolution needed for \"{text}\"')\n",
        "            return None\n",
        "        # code to look for in URI\n",
        "        cref_code = replace_with_codes(re_match, hier_labels).split(\" \")[1]\n",
        "        # print(2, cref_code)\n",
        "\n",
        "        #print(\"cref\", cref_codes)\n",
        "        # split base URI at code\n",
        "        base_URI = reg_code.split(cref_code)\n",
        "        # Check if cref_code was found in reg_code\n",
        "        if len(base_URI) > 1:\n",
        "            base_URI = base_URI[0] + cref_code + base_URI[1][0]\n",
        "        else:\n",
        "            # Handle the case where cref_code is not found\n",
        "            print(f\"cref_code '{cref_code}' not found in reg_code '{reg_code}'\")\n",
        "            return None\n",
        "\n",
        "        # get all matching codes\n",
        "        try:\n",
        "            cref_codes = [x for x in hier_codes if cref_code in x]\n",
        "            if len(cref_codes) == 1:\n",
        "                cref_codes = cref_codes[0]\n",
        "            return cref_codes\n",
        "        except:\n",
        "          print(f'error converting \"{text}\"')\n",
        "          return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "    # doc = nlp(text_to_num(reg['text_translated']))\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    crefs = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"CROSS_REFERENCE\":\n",
        "            print(\"current:\", reg['code'])\n",
        "            print(\"ent:\", ent.text)\n",
        "            cref_codes = get_cref_codes(ent.text, reg['code'])\n",
        "            if cref_codes:\n",
        "                # crefs.append(cref_codes)\n",
        "                print({'int_ref': cref_codes})\n",
        "            else:\n",
        "                print({'int_ref': str(ent.text)})\n",
        "    #print(crefs)"
      ],
      "metadata": {
        "id": "NKdeVc-leV63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity link function"
      ],
      "metadata": {
        "id": "SwvrdWuFG0gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def link_entities_rb(ent):\n",
        "    if isinstance(ent, Token):\n",
        "        # if it is a token, return nothing (invalid entity)\n",
        "        return None\n",
        "\n",
        "    # Try matching spatial elements\n",
        "    # elif ent.label_ in NER_bot:\n",
        "    elif ent.label_ in NER_spatial:\n",
        "        try: # try to see if match was found\n",
        "            # return {'element' : cand_match_exact(ent.text, kb_spatial_bot)[0]}\n",
        "            element, adjective = partial_match(kb_spatial, ent.text, \"descriptors\", list=True, exact=True)\n",
        "            return {'element' : element, **cand_match_adjective(adjective)}\n",
        "        except:\n",
        "            None\n",
        "\n",
        "\n",
        "    # Try matching building elements\n",
        "    elif ent.label_ in NER_elements:\n",
        "        # Try partial ontology URI matching (to correct for mistakes)\n",
        "        bot_candidate, adjective = partial_match(kb_spatial_bot, ent.text, \"descriptors\", list=True)\n",
        "        if bot_candidate:\n",
        "            # If there is an adjective, try to find it as either a quality or material\n",
        "            return {'element' : bot_candidate, **cand_match_adjective(adjective)}\n",
        "        else:\n",
        "            beo_candidate, adjective = partial_match(kb_BEO, ent.text, \"name\")\n",
        "            if beo_candidate:\n",
        "                # If there is an adjective, try to find it as either a quality or material\n",
        "                return {'element' : beo_candidate, **cand_match_adjective(adjective)}\n",
        "            else:\n",
        "                ifc_candidate, adjective = partial_match(kb_IFC, ent.text, \"name\")\n",
        "                if ifc_candidate: #and ifc_candidate != \"IfcSpace\":\n",
        "                    # If there is an adjective, try to find it as either a quality or material\n",
        "                    return {'element' : ifc_candidate, **cand_match_adjective(adjective)}\n",
        "\n",
        "    # Try matching materials\n",
        "    elif ent.label_ == \"material\":\n",
        "        material = cand_match_material(ent.text)\n",
        "        if material:\n",
        "            return material\n",
        "\n",
        "    # Try matching attributes either to a known list of properties, or make a new property\n",
        "    elif ent.label_ in NER_props_default:\n",
        "        return cand_match_props(ent.text)\n",
        "\n",
        "    # Try matching qualities (from rule-based quality recognition)\n",
        "    elif ent.label_ == \"QUALITY\":\n",
        "        quality = cand_match_quality(ent.text)\n",
        "        if quality:\n",
        "            return quality\n",
        "\n",
        "    # Try matching classifications\n",
        "    elif ent.label_ == \"classification\":\n",
        "        classification = cand_match_classification(ent.text)\n",
        "        if classification:\n",
        "            return classification\n",
        "\n",
        "    # Try matching use functions\n",
        "    elif ent.label_ == \"use function\": # might have to be done via a list of possible types\n",
        "        # If use functions in general are targetted, return this\n",
        "        if ent.text in kb_custom_buildingprops['props:Usefunction']['descriptors']:\n",
        "            return {'pset': 'pset:Other', 'property': 'props:Usefunction'}\n",
        "        # Else, return the mentioned use function as a string\n",
        "        else:\n",
        "            return {'pset': 'pset:Other', 'property': 'props:Usefunction', 'value': f'\"{ent.text}\"^^xsd:string'}\n",
        "\n",
        "    # Try matching quantities\n",
        "    elif ent.label_ == \"unit\":\n",
        "        quantity = cand_match_quantity(ent.text)\n",
        "        if quantity:\n",
        "            return quantity\n",
        "\n",
        "    # Try matching references and norms\n",
        "    if ent.label_ == \"CROSS_REFERENCE\":\n",
        "        # extract code using function and current reg code\n",
        "        cref_codes = get_cref_codes(ent.text, current_code)\n",
        "        if cref_codes:\n",
        "             return {'int_ref': cref_codes}\n",
        "        else:\n",
        "             return {'int_ref': str(ent.text)}\n",
        "    elif ent.label_ == \"standard\" or ent.label_ == \"reference\":\n",
        "        return {'ext_ref': ent.text}\n",
        "\n",
        "    # If there are still no matches found, find adjectives at start of token and try to find if they are in the knowledge base\n",
        "    token_deps = [token.dep_ for token in ent]\n",
        "    adjectives = \"\"\n",
        "    split_ent = ent\n",
        "    while token_deps[0] == \"amod\": #keep going until all adjective modifiers are found\n",
        "        adjectives += split_ent[0].text_with_ws\n",
        "        split_ent = split_ent[1:]\n",
        "        token_deps = token_deps[1:]\n",
        "    # Reanalyze NER of split entity and retry EL\n",
        "    if len(adjectives) > 0:\n",
        "        adj_link = cand_match_adjective(adjectives)\n",
        "        # If match is found, try entity linking again with split entities.\n",
        "        if adj_link:\n",
        "            # First try BOT (which has some problems with NER), then the rest\n",
        "            try:\n",
        "                return {'element' : find_by_keyword(kb_spatial_bot, split_ent.text, \"descriptors\", list=True, exact=True), **adj_link}\n",
        "            except:\n",
        "                split_ent_doc = nlp(split_ent.text)\n",
        "                new_ent = split_ent_doc[0]\n",
        "                # Try to match second part of entity by reiterating the function\n",
        "                new_ent_match = link_entities_rb(new_ent)\n",
        "                if new_ent_match:\n",
        "                    return {**new_ent_match, **adj_link}\n",
        "        # # alternatively, work from the split entity part\n",
        "        # split_ent_doc = nlp(''.join(token.text_with_ws for token in split_ent))\n",
        "        # for new_ent in split_ent_doc.ents:\n",
        "        #     print(new_ent.text, new_ent.label_)\n",
        "\n",
        "        #     split_link = link_entities_rb(new_ent)\n",
        "        #     if split_link:\n",
        "        #         return {**split_link, **cand_match_adjective(adjectives, kb_qualities, kb_materials)}\n",
        "\n",
        "    # If there is no match, either return nothing or return a new custom element name which will have to be defined later\n",
        "    if ent.label_ in NER_elements:\n",
        "        return {'element' : 'ex:'+''.join(token.text.title() for token in ent)}\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "rhYj1nxCkNiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"An enclosed space is located in a fire compartment.\")\n",
        "for ent in doc.ents:\n",
        "    ent_link = link_entities_rb(ent)\n",
        "    print(f'{ent.label_:25} {ent.text:30} --> {ent_link}')"
      ],
      "metadata": {
        "id": "DY6283p_CiIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji6NUXVlRIaJ"
      },
      "source": [
        "## Add entity linker to pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from spacy.tokens import Token\n",
        "from spacy import Language\n",
        "from spacy.tokens import Span, Doc\n",
        "\n",
        "# Set the extension for the new information\n",
        "Span.set_extension(\"links\", default={}, force=True)\n",
        "\n",
        "@Language.component(\"rb_entity_linker\")#, assigns=[\"doc.ents\"])\n",
        "def entity_linker(doc):\n",
        "    for entity in doc.ents:\n",
        "        entity._.set(\"links\", link_entities_rb(entity))\n",
        "    #for ent in doc.ents:\n",
        "        #ent.ent_link = link_entities_rb(ent)\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "try:\n",
        "    nlp.remove_pipe(\"rb_entity_linker\") #remove old version when rerunning code\n",
        "except:\n",
        "    None\n",
        "nlp.add_pipe(\"rb_entity_linker\", last=True)\n"
      ],
      "metadata": {
        "id": "0dSGe_TJ_Gx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "WT9hOgB-yGXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full subset validation:"
      ],
      "metadata": {
        "id": "nHQjGSvMHxSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big.iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(f\"Article {reg['code'].split('A')[1].replace('_SUB','(').replace('_','.')}): \\\"{doc}\\\"\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{str(ent.text):<25} = {ent.label_:<25} ({ent._.score:.0%}) --> {ent._.links}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "mLYKqcOFxTrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Relation Extraction & Semantic Parsing** (rule-based)"
      ],
      "metadata": {
        "id": "ES103xMF_Pqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clause extraction\n",
        "\n",
        "*Rudimentary approach, needs additional development for deployment*"
      ],
      "metadata": {
        "id": "M9qN8Wu4w15H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare checking of clauses based on index\n",
        "def split_consecutive(lst):\n",
        "    result = []\n",
        "    sublist = [lst[0]]\n",
        "\n",
        "    for i in range(1, len(lst)):\n",
        "        if lst[i] == lst[i - 1] + 1:  # If consecutive, add to sublist\n",
        "            sublist.append(lst[i])\n",
        "        else:  # Otherwise, start a new sublist\n",
        "            result.append(sublist)\n",
        "            sublist = [lst[i]]\n",
        "\n",
        "    result.append(sublist)  # Add the last sublist\n",
        "    return result\n",
        "\n",
        "# prepare checking of clauses based on index\n",
        "def split_consecutive(lst):\n",
        "    result = []\n",
        "    sublist = [lst[0]]\n",
        "\n",
        "    for i in range(1, len(lst)):\n",
        "        if lst[i] == lst[i - 1] + 1:  # If consecutive, add to sublist\n",
        "            sublist.append(lst[i])\n",
        "        else:  # Otherwise, start a new sublist\n",
        "            result.append(sublist)\n",
        "            sublist = [lst[i]]\n",
        "\n",
        "    result.append(sublist)  # Add the last sublist\n",
        "    return result\n",
        "\n",
        "# function for clause extraction\n",
        "def extract_clauses(doc):\n",
        "    clauses = [] # Initialize clauses as an empty list\n",
        "    tokens_used = [] # Initialize tokens_used list\n",
        "    # Iterate through the dependency tree to identify the target (subject) and constraint (predicate)\n",
        "    n = 1 # for counters in names\n",
        "    for token in doc:\n",
        "        if token.i not in tokens_used:\n",
        "            # Extracting the noun phrase (subject) which should be before the verb\n",
        "            if \"advcl\" in token.dep_:\n",
        "                tokens_text = ''.join([child.text_with_ws for child in token.subtree if token.i not in tokens_used])# and token.text not in [\",\", \".\"]])\n",
        "                tokens_i = [child.i for child in token.subtree if token.i not in tokens_used and not token.is_punct]\n",
        "                #print(tokens_i)\n",
        "                tokens_used += tokens_i\n",
        "                # check if negative or positive relation\n",
        "                # clauses = {**clauses, \"condition\": condition.rstrip(\" \")}\n",
        "                if \"NEN\" in tokens_text:\n",
        "                    # clauses = {**clauses, f\"EXT_REF_{n}\": tokens_text.rstrip(\" \")}\n",
        "                    clauses.append({f\"EXT_REF_{n}\": tokens_i}) # Use append to add to the list\n",
        "                    n+=1\n",
        "                else:\n",
        "                    # clauses = {**clauses, f\"CONDITION_{n}\": tokens_text.rstrip(\" \")}\n",
        "                    clauses.append({f\"CONDITION_{n}\": tokens_i}) # Use append to add to the list\n",
        "                    n+=1\n",
        "\n",
        "            elif \"subj\" in token.dep_:\n",
        "                dep_ancestors = [token.dep_ for token in token.ancestors]\n",
        "                #print(dep_ancestors)\n",
        "                if \"advcl\" not in dep_ancestors: #check whether main is not conditional statement (which is overruling)\n",
        "                    #target = \" \".join([child.ent_id for child in token.subtree])\n",
        "                    target = \"\".join([child.text_with_ws for child in token.subtree])\n",
        "                    #print(target)\n",
        "                    tokens_i = [child.i for child in token.subtree]\n",
        "                    #print(tokens_i)\n",
        "                    tokens_used += tokens_i\n",
        "                    #print(\"target:\", target)\n",
        "                    # add target to list\n",
        "                    clauses.append({f\"TARGET_UNION_{n}\": tokens_i})\n",
        "                    n+=1\n",
        "\n",
        "    rest_i = [token.i for token in doc if token.i not in tokens_used if token.text not in [\",\", \".\"]]\n",
        "    for tokens_i in split_consecutive(rest_i):\n",
        "        clause = ''.join([doc[i].text_with_ws for i in tokens_i]).rstrip(\" \")\n",
        "        if \"not applicable to\" in clause or \"not apply to\" in clause or \"not applied to\" in clause:\n",
        "            clauses.append({f\"TARGET_MINUS_{n}\": tokens_i}) # Use append to add to the list\n",
        "            n+=1\n",
        "        elif \"applicable to\" in clause or \"apply to\" in clause or \"applied to\" in clause:\n",
        "            clauses.append({f\"TARGET_UNION_{n}\": tokens_i}) # Use append to add to the list\n",
        "            n+=1\n",
        "        elif [i for i in tokens_i if doc[i].lemma_ in [\"except\", \"exception\", \"deviate\", \"deviation\"]]:\n",
        "            clauses.append({f\"TARGET_MINUS_{n}\": tokens_i}) # Use append to add to the list\n",
        "        elif [i for i in tokens_i if doc[i].lemma_ in [\"accord\", \"accordance\", \"unless\", \"contrary\", \"according\"]]:\n",
        "            clauses.append({f\"INT_REF_{n}\": tokens_i}) # Use append to add to the list\n",
        "            n+=1\n",
        "        else:\n",
        "            clauses.append({f\"CONSTRAINT_{n}\": tokens_i}) # Use append to add to the list\n",
        "            n+=1\n",
        "\n",
        "    return clauses"
      ],
      "metadata": {
        "id": "JuKSIe7IwSl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_small.iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(doc)\n",
        "    clauses = extract_clauses(doc)\n",
        "    for cl in clauses:\n",
        "        for cl_type, token_i in cl.items():\n",
        "            print(f\"{cl_type:15} = {''.join(doc[i].text_with_ws for i in token_i)}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "odCThFYtLLPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "    # doc = nlp(text_to_num(reg['text_translated']))\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(doc)\n",
        "    clauses = extract_clauses(doc)\n",
        "    for cl in clauses:\n",
        "        for cl_type, token_i in cl.items():\n",
        "            print(f\"{cl_type:15} = {''.join(doc[i].text_with_ws for i in token_i)}\")\n",
        "    # for key, value in clauses.items():\n",
        "        #\n",
        "    # for ent in doc.ents:\n",
        "    #     print(f\"{str(ent.text):<25} = {ent.label_:<25} ({ent._.score:.0%}) --> {ent._.links}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "R2wU_pvrLAX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicate extraction & Semantic parsing"
      ],
      "metadata": {
        "id": "ssERtq1hw3mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison operators"
      ],
      "metadata": {
        "id": "FQXNxIhC74jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_comparisons = {\n",
        "    '<=': ['no more than', 'no bigger than', 'not bigger than', 'no larger than', 'not larger than', 'less than or equal to', 'maximum', 'most', 'not exceeding', 'not exceed', 'limited to'], #'at most': 'sconj'\n",
        "    '>=': ['no less than', 'no fewer than', 'no smaller than', 'not smaller than', 'at least', 'more than or equal to', 'minimum'],\n",
        "    '<': ['less than', 'lower than', 'smaller than', 'fewer than', 'within', 'below'],\n",
        "    '>': ['more than', 'higher than', 'larger than', 'bigger than','exceeds', 'exceeding', 'above'],\n",
        "    '!=': ['is not', 'not', 'does not meet', 'not exactly', 'not equal to', 'not equal'],\n",
        "    '=':  ['is', 'of', 'meets', 'exactly', 'equal to', 'equals', 'exactly', 'precisely'],\n",
        "    '≈': ['approximately', 'roughly', 'close to', 'around', 'about'], # not implemented yet\n",
        "    '<>': ['not between', 'not inbetween', 'not in the range of'], # outside a range, not implemented yet\n",
        "    '><': ['between', 'inbetween', 'in the range of'] # inside a range, not implemented yet\n",
        "    # approximately?\n",
        "}\n",
        "\n",
        "\n",
        "for index, reg in df_subset_big[:10].iterrows():\n",
        "# for index, reg in subset.iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(doc)\n",
        "    for ent in doc.ents:\n",
        "        # check whether the entity is a unit\n",
        "        if ent.label_ == \"unit\":\n",
        "            # retrieve the full document up to the current unit to\n",
        "            doc_before = ''.join(str(doc[:ent.start])).rstrip(' ')\n",
        "            # find comparison operators in dictionary and return corresponding symbol if found\n",
        "            for symbol, text in dict_comparisons.items():\n",
        "                for comparison in text:\n",
        "                    # look at the end, which should be where comparisons are located\n",
        "                    if doc_before.endswith(comparison):\n",
        "                        ent._.links = {**ent._.links, 'comparison': str(symbol)}\n",
        "                        break\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"unit\":\n",
        "            print(f\"{str(ent.text):<25} = {ent.label_:<25} ({ent._.score:.0%}) --> {ent._.links}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "oDQcpMKmpviD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final function & validation"
      ],
      "metadata": {
        "id": "QpDQGc9ZLR_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for custom predicates\n",
        "def extract_predicates(clause):\n",
        "    # check all parts of the clause\n",
        "    converted_clause = []\n",
        "    for i, part in enumerate(clause):\n",
        "        # check if the part is an entity (dictionary)\n",
        "        if isinstance(part, dict):\n",
        "            if part.get('element'):\n",
        "                # custom spatial predicates (containment, adjacencies)\n",
        "                if \"in \" in clause[i-1]:\n",
        "                    if 'FireCompartment' in part['element']:\n",
        "                        converted_clause.append({'containment_path': 'ex:locatedInCompartment', 'containment_class': part['element']})\n",
        "                    elif 'Space' in part['element']:\n",
        "                        converted_clause.append({'containment_path': 'bot:hasSpace', 'containment_class': part['element']})\n",
        "                    else:\n",
        "                        converted_clause.append(part)\n",
        "                elif \"adjacent\" in clause[i-1]:\n",
        "                    if 'FireCompartment' in part['element']:\n",
        "                        converted_clause.append({'adjacent_path': 'ex:adjacentCompartment', 'adjacent_class': part['element']})\n",
        "                    elif 'Space' in part['element']:\n",
        "                        converted_clause.append({'containment_path': 'bot:adjacentZone', 'containment_class': part['element']})\n",
        "                    else:\n",
        "                        converted_clause.append({'containment_path': 'bot:adjacentElement', 'containment_class': part['element']})\n",
        "                else:\n",
        "                    converted_clause.append(part)\n",
        "            else:\n",
        "                converted_clause.append(part)\n",
        "\n",
        "    return converted_clause\n",
        "\n",
        "from itertools import groupby\n",
        "def merge_consecutive_dicts(lst):\n",
        "    merged_list = []\n",
        "    for is_dict, group in groupby(lst, key=lambda x: isinstance(x, dict)):\n",
        "        if is_dict:\n",
        "            merged_dict = {}\n",
        "            for d in group:\n",
        "                merged_dict.update(d)\n",
        "            merged_list.append(merged_dict)\n",
        "        else:\n",
        "            merged_list.extend(group)\n",
        "\n",
        "    # Check if the list is empty before accessing element 0\n",
        "    if merged_list:\n",
        "        return merged_list[0]\n",
        "    else:\n",
        "        # Return an appropriate value if the list is empty\n",
        "        return {}  # or None, or whatever is suitable for your use case\n",
        "\n",
        "# function for finding logical operators\n",
        "def extract_logic(clause, clause_type):\n",
        "    l_ents = []\n",
        "    l_nest_ents = []\n",
        "    # find all parts of converted clause\n",
        "    for i, part in enumerate(clause):\n",
        "        # find all entities (which are contained as dictionary items)\n",
        "        if isinstance(part, dict):\n",
        "            # try to see if the previous item is a predicate (which is a string)\n",
        "            if isinstance(clause[i-1], str):\n",
        "                # try to find predicate, which should be the previous doc item\n",
        "                try:\n",
        "                    predicate = clause[i-1]#.strip(' ')\n",
        "                    # find logical operators, and map them with their equivalent logical operator or SPARQL operator\n",
        "                    n = 1 # counter to avoid duplicates\n",
        "                    if 'or ' in predicate or 'or;' in predicate:\n",
        "                        # clause[i-1].replace('or', '')\n",
        "                        if \"TARGET\" in clause_type:\n",
        "                            op = f\"UNION_{n}\"\n",
        "                        else:\n",
        "                            op = f\"OR_{n}\"\n",
        "                        n+=1\n",
        "                        if l_nest_ents: # for chained logical operators\n",
        "                            l_ents.append({op: l_nest_ents + [part]})\n",
        "                            l_nest_ents = []\n",
        "                        else: # for simple logical operators\n",
        "                            l_nest_ents = [clause[i-2], part]\n",
        "                            del l_ents[-1]\n",
        "                            l_ents.append({op: l_nest_ents})\n",
        "                            l_nest_ents = []\n",
        "                    elif 'and ' in predicate or 'and;' in predicate:\n",
        "                        if \"TARGET\" in clause_type:\n",
        "                            op = f\"INTERSECT_{n}\"\n",
        "                        else:\n",
        "                            op = f\"AND_{n}\"\n",
        "                        n+=1\n",
        "                        if l_nest_ents: # for chained logical operators\n",
        "                            l_ents.append({op: l_nest_ents + [part]})\n",
        "                            l_nest_ents = []\n",
        "                        else: # for simple logical operators\n",
        "                            l_nest_ents = [clause[i-2], part]\n",
        "                            del l_ents[-1]\n",
        "                            l_ents.append({op: l_nest_ents})\n",
        "                            l_nest_ents = []\n",
        "                    elif 'not ' in predicate:\n",
        "                        if \"TARGET\" in clause_type:\n",
        "                            op = f\"MINUS_{n}\"\n",
        "                        else:\n",
        "                            op = f\"NOT_{n}\"\n",
        "                        n+=1\n",
        "                        if l_nest_ents: # for chained logical operators\n",
        "                            l_ents.append(l_nest_ents + [{op: part}])\n",
        "                        else:\n",
        "                            l_ents.append({op: part})\n",
        "                    elif ',' in predicate or ';' in predicate:\n",
        "                        if l_nest_ents:\n",
        "                            l_nest_ents.append(part)\n",
        "                        # if this is the first item in the nested list, add previous item (before the comma/semicolon) as well and remove it from flattened list\n",
        "                        else:\n",
        "                            l_nest_ents = [clause[i-2], part]\n",
        "                            del l_ents[-1]\n",
        "\n",
        "                    else:\n",
        "                        # add predicate to text (except for logical operators)\n",
        "                        l_ents.append(clause[i-1])\n",
        "                        # add entity to text\n",
        "                        l_ents.append(part)\n",
        "                except:\n",
        "                    l_ents.append(part)\n",
        "            else:\n",
        "                l_ents.append(part)\n",
        "    return l_ents\n",
        "\n",
        "def link_comparisons(predicate, ent):\n",
        "    # find comparison operators in dictionary and return corresponding symbol if found\n",
        "    if ent.label_ == \"unit\":\n",
        "        for symbol, text in dict_comparisons.items():\n",
        "            for comparison in text:\n",
        "                # look at the end, which should be where comparisons are located\n",
        "                if predicate.rstrip(' ').endswith(comparison):\n",
        "                    # if found, add comparison operator to entity link and remove it from the text\n",
        "                    predicate = predicate.replace(predicate, ' ')\n",
        "                    ent._.links = {**ent._.links, 'comparison': str(symbol)}\n",
        "                    break\n",
        "    return predicate, ent\n",
        "\n",
        "# function for replacing entities with linked info\n",
        "def convert_doc(doc):\n",
        "  # Initialize an empty list to hold the modified tokens\n",
        "    converted_doc = []\n",
        "    # Set to track which tokens have already been replaced by span labels\n",
        "    replaced_tokens = set()\n",
        "    # Iterate over all tokens in the document\n",
        "    for token in doc:\n",
        "        # Check if the token is part of any entity (span)\n",
        "        in_span = False\n",
        "        for ent in doc.ents:\n",
        "            if token.i == ent.start:  # token is part of the span\n",
        "                # try to find predicate, which should be the previous doc item\n",
        "                try:\n",
        "                    predicate = converted_doc[-1]#.rstrip(' ')\n",
        "                    # try to link predicates and entities\n",
        "                    predicate, ent = link_comparisons(predicate, ent)\n",
        "\n",
        "                    # try to replace predicate or remove it if it is empty\n",
        "                    if not predicate or predicate == ' ':\n",
        "                        del converted_doc[-1]\n",
        "                    else:\n",
        "                        converted_doc[-1] = predicate\n",
        "                except:\n",
        "                    None\n",
        "                # replace entities with corresponding entity links\n",
        "                if token not in replaced_tokens:\n",
        "                    converted_doc.append(ent._.links)  # Replace with span label\n",
        "                in_span = True\n",
        "                break\n",
        "            elif token.i > ent.start and token.i < ent.end:\n",
        "                in_span = True\n",
        "\n",
        "        # If the token is not part of any span, it should be checked for predicates\n",
        "        if not in_span:\n",
        "            # try to add tokens as normal text\n",
        "            try:\n",
        "                converted_doc[-1] += token.text_with_ws\n",
        "            except:\n",
        "                converted_doc.append(token.text_with_ws)\n",
        "    return converted_doc\n",
        "\n",
        "def extract_relations(doc):\n",
        "    # find clauses\n",
        "    clauses = extract_clauses(doc)\n",
        "    extracted_dict = {}\n",
        "    # extract entities & predicates from clauses\n",
        "    for cl in clauses:\n",
        "        for cl_type, token_i in cl.items():\n",
        "            # get new doc span for clause\n",
        "            cl_doc = doc[token_i[0]:token_i[-1]+1]\n",
        "            # replace entities with linked info & find comparisons (cardinality/quantifiers should also be added here!)\n",
        "            converted_clause = convert_doc(cl_doc)\n",
        "            # extract logical structure\n",
        "            converted_clause = extract_logic(converted_clause, cl_type)\n",
        "            # extract predicates\n",
        "            converted_clause = extract_predicates(converted_clause)\n",
        "            # join consecutive entities\n",
        "            converted_clause = merge_consecutive_dicts(converted_clause)\n",
        "            # add clause extraction to dictionary (if non_empty)\n",
        "            if converted_clause:\n",
        "                extracted_dict[cl_type] = converted_clause\n",
        "\n",
        "    return extracted_dict"
      ],
      "metadata": {
        "id": "DYXev_8i8Mdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_small.iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(doc)\n",
        "    extracted_dict = extract_relations(doc)\n",
        "    for key, value in extracted_dict.items():\n",
        "        print(f\"{key:15} = {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ySc3dgIh8qYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, reg in df_subset_big.iterrows():\n",
        "    doc = nlp(reg['text_translated'])\n",
        "    print(f\"Article {reg['code'].split('A')[1].replace('_SUB','(').replace('_','.')}): \\\"{doc}\\\"\")\n",
        "    extracted_dict = extract_relations(doc)\n",
        "    for key, value in extracted_dict.items():\n",
        "        print(f\"{key:15} = {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "kqO5PO90QqHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Final information extraction**\n",
        "\n",
        "Validation using small subset"
      ],
      "metadata": {
        "id": "ga7XycVMA31w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "xACfJF9fOlmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final pipeline order should be:\n",
        "```\n",
        "['tok2vec',\n",
        " 'tagger',\n",
        " 'parser',\n",
        " 'attribute_ruler',\n",
        " 'lemmatizer',\n",
        " 'gliner_spacy',\n",
        " 'cref_recognizer',\n",
        " 'quantity_recognizer',\n",
        " 'quality_recognizer',\n",
        " 'rb_entity_linker']\n",
        "```"
      ],
      "metadata": {
        "id": "FaMo6dJ-0ths"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract information\n",
        "\n",
        "*Conditional information extraction and cross-reference contextualization not yet implemented!*"
      ],
      "metadata": {
        "id": "Yh-Cu5argY2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "output should have a structure like:\n",
        "```\n",
        "[ { 'INFO': {\n",
        "        'reg_CURIE': '__:___',\n",
        "        'label': 'Article 00.00(00)',\n",
        "        'seeAlso': 'https://_____',\n",
        "        'text_en' : '___',\n",
        "        'text_original' : '___'},\n",
        "    'TARGETS'  : {\n",
        "        ...},\n",
        "    'CONSTRAINTS': {\n",
        "        ...}\n",
        "    'CROSS_REFERENCES' : [\n",
        "      ...\n",
        "    ]\n",
        "    'EXT_REFERENCES': [\n",
        "        ...\n",
        "    ]\n",
        "        },\n",
        "  { 'INFO': {\n",
        "        'code': '__:___',\n",
        "        'label': 'Article X.X/X',\n",
        "        'seeAlso': 'https://_____',\n",
        "        'text_en' : '___',\n",
        "        'text_original' : '___'},\n",
        "    'CONDITION_1'  : {\n",
        "        'TARGETS'  : {\n",
        "            ...},\n",
        "        'CONSTRAINTS': {\n",
        "            ...}}},\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "MPT4v5y96Bth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(df_reg):\n",
        "    extract = []\n",
        "    # extract information and store in a list with a dictionary for each regulation\n",
        "    for index, reg in df_reg.iterrows():\n",
        "        # extract article (and subarticle) label from code as Article 00.00(0)\n",
        "        if 'SUB' in reg[\"code\"]:\n",
        "            label = 'Article '+reg[\"code\"].split(\"_A\")[1].replace('_SUB', '(').replace('_', '.')+')'\n",
        "        else:\n",
        "            label = 'Article '+reg[\"code\"].replace('_', '.')\n",
        "        extract.append({\n",
        "            # add meta information\n",
        "            'INFO': {\n",
        "                'reg_CURIE': 'bbl:'+reg['code'],\n",
        "                'label': label,\n",
        "                'seeAlso': reg['URL'],\n",
        "                'text_en': reg['text_translated'],\n",
        "                'text_original': reg['text_original'],\n",
        "            },\n",
        "        })\n",
        "\n",
        "        global current_code  # needed for cross-reference linking\n",
        "        current_code = reg[\"code\"]\n",
        "        # extract information (tokens, POS, dependency tree, NER labels, linked entities, linked predicates, relations)\n",
        "        doc = nlp(reg[\"text_translated\"])\n",
        "        extracted_information = extract_relations(doc)\n",
        "\n",
        "\n",
        "\n",
        "        # figure out if target and/or constraints are present, and if so, add empty items to the dictionary\n",
        "        for key, values in extracted_information.items():\n",
        "            if \"TARGET\" in key and not extract[-1].get('TARGETS'):\n",
        "                extract[-1]['TARGETS'] = {}\n",
        "            elif \"CONSTRAINTS\" in key and not extract[-1].get('CONSTRAINTS'):\n",
        "                extract[-1]['CONSTRAINTS'] = {}\n",
        "            elif \"INT_REF\" in key and not extract[-1].get('INT_REFERENCES'):\n",
        "                extract[-1]['INT_REFERENCES'] = []\n",
        "            elif \"EXT_REF\" in key and not extract[-1].get('EXT_REFERENCES'):\n",
        "                extract[-1]['EXT_REFERENCES'] = []\n",
        "            elif values.get('ext_ref'):\n",
        "                extract[-1]['EXT_REFERENCES'] = []\n",
        "\n",
        "            # INCLUDE CODE FOR CONDITIONS HERE\n",
        "\n",
        "        n = 1 # counter to make sure dictionary keys are unique\n",
        "        # fill dictionary with corresponding information\n",
        "        for key, values in extracted_information.items():\n",
        "            if \"TARGET_UNION\" in key:\n",
        "                # remove first layer of OR logic, if present\n",
        "                # if \"UNION\" in values and len(values) == 1:\n",
        "                    # print(1, values)\n",
        "                    # values = values.get('UNION')\n",
        "                extract[-1]['TARGETS'] = values #{**extract[-1]['TARGETS'], f'UNION_{n}': values}\n",
        "                n+=1\n",
        "            elif \"TARGET_MINUS\" in key:\n",
        "                extract[-1]['TARGETS'] = {**extract[-1]['TARGETS'], f'MINUS_{n}': values}\n",
        "                n+=1\n",
        "            elif \"CONSTRAINT\" in key:\n",
        "                extract[-1]['CONSTRAINTS'] = values\n",
        "            elif \"INT_REF\" in key:\n",
        "                extract[-1]['INT_REFERENCES'].append(values)\n",
        "            elif \"EXT_REF\" in key:\n",
        "                extract[-1]['EXT_REFERENCES'].append(values)\n",
        "            elif values.get('ext_ref'):\n",
        "                extract[-1]['EXT_REFERENCES'].append({'ext_ref': values['ext_ref']})\n",
        "\n",
        "        # display extracted information for current regulation\n",
        "        print(json.dumps(extract[-1], indent=4))\n",
        "    return extract"
      ],
      "metadata": {
        "id": "w7B3PHOsrPml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_info_subset_small = extract_information(df_subset_small)"
      ],
      "metadata": {
        "id": "ca7iryXmyBLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_info_subset_big = extract_information(df_subset_big)"
      ],
      "metadata": {
        "id": "oFrjY6AqyAke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to JSON"
      ],
      "metadata": {
        "id": "dOfm28o1gVhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"output/regulatory_information_subset_small.json\", 'w') as json_file:\n",
        "    json.dump(reg_info_subset_small, json_file)\n",
        "with open(\"output/regulatory_information_subset_big.json\", 'w') as json_file:\n",
        "    json.dump(reg_info_subset_big, json_file)"
      ],
      "metadata": {
        "id": "B3obNuyOgT8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}